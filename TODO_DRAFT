                                 Muldis::D
                                TODO DRAFT
---------------------------------------------------------------------------

This TODO_DRAFT file is an overflow destination of sorts for the TODO file,
and it contains a large amount of brainstorming, some verbose, in the guise
of TODO items, but much of this is highly unlikely to be done without major
changes, and in any event it is considered inappropriate for release
distributions due at least to its great rate of growth and low signal to
noise.  This TODO_DRAFT file only exists in the Muldis D version control
repository, not in the packaged Muldis D distributions on CPAN, so it is
still available for interested parties to read but doesn't waste the
resources of everyone else.

The following list is loosely ordered by priority, and is organized into
groups by approximate subject area, but list items may
actually be addressed in a different order.  There is no specific time
table for these items; they are simply to be done "as soon as possible".

----------

* BEFORE DOING ANY OTHER LARGE THING, CREATE A GIT BRANCH THAT I JUST DO A
PILE OF CHANGES IN TOGETHER, AND THEN SEPARATELY I CAN MANUALLY MERGE BITS
OF THAT TO TRUNK WHEN READY TO DO A SERIES OF MORE ORDERLY RELEAESES.
- I SUPPOSE PROBABLY BEST TO DO THE BRANCH ON GITHUB'S END?  RESEARCH!
- I KNOW I WANT TO SEE THE BRANCH AND TRUNK SAME TIME BOTH THERE AND HERE.

* It must be mandatory for each grouping () paren pair in an expression to
have exactly one sub-expression which is not explicitly named with ::= because that
is the root sub-expression of the grouping () ... or alternately we change
the definition of ["" ::=] to simply indicate which sub-expression in a
grouping () is its root; but then we actually should have a keyword,
and maybe "<--" could be that keyword ... the keyword would be optional
and the simple absense of leading ::= from one sub-expr marks it as main;
but if all sub-exprs are named then the keyword is mandatory.

* Eliminate uses of |,&,- to mean union/or,intersection/and,minus with
respect to type defs or {schema object, param/attr} alias declarations.
- We should discourage aliasing even if supporting it, and the best way is
to not provide special/terse syntax for doing so that isn't generalizable.
- For schema object aliasing, people can just do this:
    sum ::= add ::= "+" ::= function ...;
... assuming all of said aliases are okay to have identical signatures/etc.
- In practice, explicit union/intersect/minus types probably aren't that
common except for say enum declarations, so just support those with the
likes of:
    Int_or_Text ::= Universal where (.0 isa $Integer or .0 isa $Text);
- Provide appropriate generic expression syntax / functions such that one
can use them effectively for declaring routine signatures/etc without those
needing special syntax.
- As such, the general form of the declaration of function foo would be
in terms of 5 mutually isolated (having their own expression node
namespaces) generic expression trees, any of which or parts thereof may
optionally be factored out into (the bodies of) other functions;
within each tree, "_" is a keyword / the default name of the (locally)
singleton expression node representing the topical input value of the
expression aka its sole parameter; these are those 5 trees:
    1. A e(Boolean<--Universal) that declares the (usually Tuple) dispatch
    type of foo's input parameter; that is, a call to "foo(x)" will cause
    this particular foo to be dispatched to if True<--e(x) assuming also
    this particular foo is the first so-matching one found during dispatch.
    2. A e(Universal<--Universal) that normalizes foo's input parameter
    from whatever conceptual multiple parameter aliases may exist to one.
    3. A e(Boolean<--Universal) that declares any further constraints on
    the input parameter, eg that denominator may not be zero, when we want
    to explicitly fail certain function calls rather than the dispatcher
    simply falling back to no-such-routine when you call say divide(4,0);
    a result of False means fail and True means accept.
    4. A e(Universal<--Universal) that declares the main body of the
    function, that which does the real work the function exists for.
    5. A e(Boolean<--Universal) that declares foo's result type; for a
    function this is the other part of foo's main signature besides #1.
- A general procedure declaration would follow a similar pattern, except
that there wouldn't be a #5 so to speak and inverse expressions may be
necessary (ideally generated) for #2 etc considering the common practice
of subject-to-update parameters; procedures need more thought.
- Generic system-defined functions such as Tuple_follows_template() can
provide conveniently short syntax for signature declarations, including
whether each attr is mandatory or optional, what aliases an attr may
have, whether the attr list is exhaustive or a subset, whether attrs have
positional-alike/integer names, etc.
- Generic system-defined functions for testing multiple "isa" on the same
value can be given, eg "v is_any_of {$Int,$Text}", which also works for
declaring enumerated types, or "v is_all_of {$Odd,$HundredPlus}" etc.
- Remove ":" from function declaration as the line between function head
and body is more fuzzy, but have some terse way of marking each part of the
function.  Also make bounding parens optional for the 5 expression trees so
eg you don't have to say "(5)" when "5" would do; the bounding parens would
just be needed when it isn't otherwise clear where the expression ends and
the next part of the function declaration or next schema object declaration
begins.  These should all work equally well when a function declaration is
embedded within another value expression, such as for the meat of generic
relational restrictions.  Perhaps something like this:
    foo ::= function --> (_ isa $Integer) <-- (_ follows ...) : (#_);
    foo ::= function to (_ isa $Integer) from (_ follows ...) how (#_);
... noting that the 5 parts may come in any order and are all optional.
Clearly this will take a lot of thought.
- See Perl 6 design for guidance, it already does a lot of that stuff.
- Have function shorthand eg "tup?.attr" that returns a VoidLike value
rather than dying if the tuple doesn't have an attribute of that name.
- Maybe go further and make "?" a parser-recognized meta-operator, similar
to how "!" is; so while "bar !x foo" means "not(bar x foo)",
"bar ?. foo" means "has_attr(bar,$foo) ?? bar.foo !! :$Void" or some such.
- So then defaulting an optional parameter can be like "_?.mynum // 3".
- Have shorthand to choose, among multiple possible attribute names, to
determine under which name, if any, the attribute exists, take its value;
it would also assert no more than one of the attr names exists at once.
- We can also say that technically the only actual required part of a
function definition is #4 the main body expression.

- Add keyword "type" that is like "function" but just takes a body def expr
where that body is a predicate, and implicitly is Universal-->Boolean.

* Change how routines work so that the actual sole input argument may be a
Universal rather than having to be a Tuple.  Also create a new core type
that represents the actual argument which has both an Array and a Tuple
component, which is the canonical way to represent effective argument lists
that have either or both ordered and named arguments.  So we then eliminate
the kludge where ordered parameters are represented by Tuple attributes
named "0" and "1" etc, which is partly bad due to being base-10-centric.
Along with that new type is new special syntax for selecting a value of it,
where that syntax resembles the existing complexities of an argument list.
Then alter the general syntax of a routine call to be more like that of a
Capsule value selector ("=>"), in that it takes the appearance of an infix
operator, eg "<-" (like "." does), like this:
  rtn <- args
eg:
  $div <- :( x, y )
where that particular example also has the shorthand:
  div(x,y)
or:
  x div y
Within a routine, the sole parameter is named "_" and where the new special
type is used, the value composes both ArrayLike and TupleLike, so you can
access the actual parameters like this:
  _.[0]
or
  _.foo
... or as ".[0]" or ".foo" respectively.
There would also be normal operators for merging a Tuple+Array into one of
the new types, or extracting the whole Tuple or Array component.
Obviously the default value of the new type is an empty array+empty tuple.
The "rtn" input to "<-" could also be an anonymous function declaration,
which in its general form is just a body-defining expression; but in that
case, for legibility, we may want to offer "->" as well so one can say:
  args -> rtn
Actually, we may want the "args -> rtn" form to be the "normal" general
form, assuming it is mainly used with anonymous routines, and that use with
named routines is uncommon, and then simply not support "<-" at all.
We will likely replace "v->op(...)" with some other syntax then or drop it.
We will want to use Perl 6 for guidance on how this type/etc works.
Or use "args --> rtn" and leave "v->op(...)"?  But what is more common?
In Perl 6 ":(...)" creates a Signature while "\(...)" makes a Capture.

* Formally make all functions be unary Universal-->Universal and deal with
it re consequences on relationship to other languages or whatever.
- This gives Muldis D more theoretical purity and means that special types
for representing arguments or parameter decls don't have to be low-level.
- But if all functions are unary, how to represent nullary avoiding
recursion eg of the definition of its placeholder argument?
Maybe one way is to say a function may be invoked without an explicit
argument and in that case it is the empty LL_List that the function gets.
- All Low_Level.foo will explicitly not use the new input argument type
that combines Array+Tuple etc, only non-Low_Level foo would.
Instead they would explicitly use LL_List of particular arity for any
functions that conceptually take multiple arguments, and just the argument
directly when just one.  As such, all invocations any LL_foo would by
necessity use the general form of function calling eg args-->func or
func(*args) which would be another way to discourage direct use of LL_foo
in general code.
- All non-Low_Level.foo explicitly will use the special args type.
- The new args type definitely will be a Capsule, a SC_foo to be specific.
- All invocation syntax like "x op y" or "op x" or "op(x,y)" or "op(...)"
will always mean use the new args type, and so they'll only be for non-LL.
- All predicate functions intended to be used in the position of defining a
type must be bare Universal-->Boolean; that is, the type of the "topic" is
NOT a SC_Func_Args in the general case.
- Similarly, all predicate functions used in defining routine headings,
that is dispatch type, input normalizer, input constraint, result type,
for these type type of "topic" is also NOT a SC_Func_Args in the general.
- So, Universal."=" will have to be changed to be a wrapper of LL_same
rather than a synonym of it, since the former will use a SC_Func_Args while
the latter will not.  Some other similar splits would happen as approp.

* Make backslash "\" only mean escaping within delimited strings; outside
said it has no mnemonic and can be used as generally as any symbolic chars.
However some special syntax starts with a \, eg relation literals, which
will override in the case of ambiguity.

* Also, make it so function decl type is essentially just a tuple of
traits, where body and params decl etc are just traits same as
is-commutative or default or whatever.  Every trait is a key-value pair
with the key being the trait name and the value either being a boolean or
some other type, typically an expression node set.

* Let "isa" have prefix shorthand so "isa $foo" means "topic isa $foo".

* First task - we need to build a list of expression and stmt node types.
- Topic_Expr_Node - represents the real function input argument
- Resolved_Expr_Node - represents the "resolved" keyword; turns Ident to Ref
- ...

* Function call syntax:
    args --> rtn
    \(x,y) --> $"+"
    $SC_Func_Args=>\%{ordered=>[x,y],named=>\%{}} --> $"+"
    "+"(x,y)
    x->"+"(y)
    x + y
    div(x,y,round_meth:$=>To_Zero)
    \(x,y) --> (function (Integer <-- Integer, Integer) :
    \(x,y) --> (function to (isa $Integer) from (isa \:(Integer,Integer))
    \(x,y) --> (function --> (isa $Integer) <-- (isa \:(Integer,Integer))
    42 --> $some_func_taking_Integer_directly_without_SC_Func_Args_wrapper

* Function declare syntax general form:
    0. keyword "function" followed by 0 or 1 of each of next in any order:
    1. keyword "to" or "-->" followed by predicate expr taking "_" as input
    ...

* Schema objects can be easily indexed for fast(er) dispatch.  To be
specific, we can pre-make a map from each abbreviation (particularly the
fully-unqualified) object names to a list of the fully-qualified names that
said short one is short for, so an actual scan isn't needed per dispatch.
We rebuild/alter the map whenever a data definition action occurs.
Similarly, there is also a map for each fully-qualified schema object name
and the data types of its arguments, though that one might be lazily built.

* Update ordering type per change to Perl 6 (instead of [Inc|Dec]rease):
https://github.com/perl6/specs/commit/5e2c4205
"These operators compare their operands using numeric, string, or C<eqv>
semantics respectively, and if the left operand is smaller, the same,
or larger than the right operator, return respectively C<Order::Less>,
C<Order::Same>, or C<Order::More> (which numerify to -1, 0, or +1,
the customary values in most C-derived languages)."

* "ideomatic" - "Peculiar to or characteristic of a given language."

* Consider signing up with https://www.gittip.com or other such venues as a
way to collect miscellaneous revenue to support Muldis D / etc development.
Just start this after I make my first deliverable and so supporters have
something already in exchange.  Would treat as business income by way of
Muldis Data Systems.  Not sure if I can identify Canadians from the givers
but if I can I'd treat part of theirs as GST collected.
See also communities such as https://www.gittip.com/for/perl/ .
See http://www.dagolden.com/index.php/2325/why-i-finally-joined-gittip-and-why-you-should-too/
and other blog posts it links to.

* How to make a generic join-like operator that is like SQL's and not like
normal natural join.  SQL's is fundanentally a cartesion product and each
input relation has its own namespace in the output; in Muldis D we will
have to specify said namespace as data, it isn't just gleaned from the
expression or variable names in source code, we do this with a tuple.
  :%{artists:db.artists} xjoin :%{albums:db.albums}
Each tuple of the result has 2 tuple-typed attrs named {artists,albums}
and the values are the tuples of the source relations.
One can then use where() to define a generic join condition like SQL eg:
  artists_tup xjoin albums_tup where :(.0.albums.artist_id = .0.artists.id)
Or another signature of xjoin itself could take more than 2 args where a
third etc provides join conditions etc; but then xjoin(a,b,c) call needed.
To support this in an N-ary fashion, xjoin has 2 signatures (both marked
commutative) which are tup*rel and rel*rel restricted such that a tup must
be unary with a relation-type attr and any rel may be N-ary where every
attribute is tuple-typed, and thus everything should just work.
The output of xjoin is the same type as said rel input type.
In the standard scenario xjoin requires that the primary attr names of all
inputs are mutually distinct since the result is a disjoint union of them.
So this should work:
  [xjoin] {a, b, c, d}
or
  a xjoin b xjoin c xjoin d
This is all basically an alternate way of doing:
  (a wrap all as a) join (b wrap all as b) etc

* It should be worth pointing out explicitly that when I write SQL I
automatically keep wanting to put the WHERE clause earlier in an expression
than SQL allows it to go, eg:
  update x where y set z;
or:
  select ... from x where y join a using (b) ...
... and so on.  And in Muldis D such intuitive actions are actually valid.
So an example of how Muldis D makes common tasks easier than SQL,
especially ad-hoc queries.

* Apparently Perl 6 doesn't have union types after all, eg Int|Str where a
type name could go.  Closest is "subset IntStr of Any where Int|Str".  See
http://www.nntp.perl.org/group/perl.perl6.compiler/2014/03/msg9483.html .
So | is always a junction.

* Note the following change in Perl 5.18.0; I should do the same in the STD
parser, that is treat all 11 of the same characters as white space such
that literal occurrences of said are treated as equivalent to a SPACE etc.
"When a regular expression pattern is compiled with /x, Perl treats 6
characters as white space to ignore, such as SPACE and TAB. However,
Unicode recommends 11 characters be treated thusly. We will conform with
this in a future Perl version. In the meantime, use of any of the missing
characters will raise a deprecation warning, unless turned off.
The five characters are: U+0085 NEXT LINE U+200E LEFT-TO-RIGHT MARK
U+200F RIGHT-TO-LEFT MARK U+2028 LINE SEPARATOR U+2029 PARAGRAPH SEPARATOR"

* Create another Git repository and release series or three that just
contain Muldis D code, these being analagous to Larry Wall's STD.pm / Perl
6 grammar written in Perl 6, and the Perl 6 Prelude and standard libraries
written in Perl 6, and the Perl 6 test suite written in Perl 6.
- Contained therein is all of the "official" muldis.com Muldis D packages,
just as PT_STD text files, and the Muldis D compilers written in PT_STD,
and the Muldis D official test suite written in PT_STD.
- This release series is *not* expected to be released as an executable
product by itself; rather it is more of a data product whose files are then
copied into other projects that actually have the bootstrap or framing or
whatever Perl or whatever code so that the other projects are executables;
they include the as-data files to save themselves work and test themselves.
- Said other Perl/etc projects would synchronize themselves periodically
with the PT_STD-only project for releases, but the latter has no Perl/etc
in it.
- Think of the PT_STD-only product like the data files that the Unicode
consortium releases, and the latter being like Perl etc which bundles said
Unicode data files as effectively parts of the Perl/etc executables.
- Maybe the modules, compilers, and test suite will all live in one
repository, but those parts may be released as separate product distros.
- The repository can have lib/, bin/, and t/ dirs to group the above into.
- The above PT_STD-only alone probably would *not* be distributed on CPAN.
- Even http://www.unicode.org/Public/UNIDATA/ has both spec-data+test fils.
- Maybe name this repository "Muldis-D-Standard"?
- This new repository doesn't have to be held back for any work in existing
repositories such as "Muldis-D" or whatever, and in fact would probably be
the development process leader for a little while.

* Create another Git repository and release series "Muldis-D-Ref-Perl5" or
such that would have the Perl 5 based reference implementation, and it is
bundled into this that "Muldis-D-Standard" would be distributed on CPAN,
and it is this that would actually run.  The "Muldis-D-Ref-Perl5" would
include both a module for embedding in Perl programs ala DBI which succeeds
"Muldis-Rosetta" and also a script for compiling standalone Muldis D
programs.  Any or all of the Perl in this distro may be generated from
PT_STD code in Muldis-D-Standard, or at least the script and shared parts
would be, but the public parts of the module might not be.
The bundled DBD-alike module may have the name "Muldis::D::RefEng".
- Yet another repository, say "Muldis-D-Perl5", can specify HD_Perl5_STD
as "Muldis-D" soon wouldn't, and also specify a common Perl 5 API for both
"Muldis::D::RefEng" and other DBD analogies sharing the API to follow.

* PRIORITY LIST:
1.  Split off Perl stuff to separate distros/etc, chg language name fmts.
2.  Canonicalize low-level tp-sys so that simple "=" just works everywhere.
3.  Redefine all routines to have exactly 1 parameter, tuple-typed.
    - Have minimal other changes, including minimal syntax changes.
    - Syntax changes mainly re pass/def tup literally or not ala Perl 6.
4.  ...

* Add "is deprecated" flags to materials so that definers can say in
advance that something is going away in future versions before it actually
does, and users can be warned.  I think Perl 6 or Rakudo has it for
subroutines and such.

* Add references to or adoptions of ISO/IEC 11404:2007(E) "Information
technology -- General-Purpose Datatypes (GPD)" which could be very useful.

* Add mention of http://rubydoc.info/github/blambeau/alf/master/Alf .

* See http://scale-out-blog.blogspot.ca/2014/02/why-arent-all-data-immutable.html .

* See http://docs.python.org/2/library/datatypes.html .  Also, Python uses
the type name Counter for what Muldis D calls Bag it seems.
Also a lot of Python and Perl 6 type names are the same, suggesting influence.

* Apparently names like_this are called "snake case".
https://en.wikipedia.org/wiki/Snake_case .

* Apparently this is valid in Perl 5 (trailing ::), and can be disambiguating:
    Package::->method();

* Perl 5.19.x/20.x add native sub signatures.

* See https://github.com/wbraswell/rperl/ for something that might be
relevant to translating Perl code to Muldis D code.
Or I can support RPerl as a compilation target; faster subset of Perl.
See also http://perl11.org and stuff.

* See newish DBMS FoundationDB that's about widely-scalable ACID supporting
both relational and non-relational.  A prime DBMS candidate to target, may
possibly be easier due to its design to support than Postgres, or maybe
not; it is also closed source.  Supports Linux (main) + Mac and Win (dev).
See also https://foundationdb.com/blog/call-me-maybe-foundationdb-vs-jepsen
for some stuff on how it works, and also the separate party Jepsen.

* See also Google's Spanner and F1 which are much the same, but internal.

* I probably want to use LMDB http://symas.com/mdb/ for my second
implementation, or alternately support it as an option in the first/ref.
There is lots of precedents for doing this too, eg SQLite4, multi "nosql".

* Use same node type to represent bounding parens as used for synonyms.
Eg `foo ::= bar ::= 3` the foo is a synonym/aliasing node.
But aliasing nodes can have generated node names same as other anon exprs
so eg `1 + ((2+3))` the outer paren is an aliasing/syn node.
Parens themselves are general metadata that can go on any kind of node,
eg, is this surrounded by explicit parens in the text source or not.
Whether explicit node names are quoted or not is also metadata.

* Postgres 9.3 has ddl event triggers so eg you can block accidental
drop table and such if you want with a trigger.

* The Oracle foreign data wrapper for Postgres is declared stable now;
see http://oracle-fdw.projects.pgfoundry.org .

* Postgres 9.4 supports this apparently:
coalesce(min(val) filter (where val > 0), 0)
http://www.depesz.com/2013/07/23/waiting-for-9-4-implement-the-filter-clause-for-aggregate-function-calls/

* Note http://rhaas.blogspot.ca/2014/03/vacuum-full-doesnt-mean-vacuum-but.html
that VACUUM FULL is actually quite different than VACUUM, and in particular,
plain VACUUM is needed to accomplish certain things; generally plain VACUUM
is what you want during regular maintenance, unless eg your relation is
mostly empty and you want to return the space to the operating system;
VACUUM FULL does not deal with the id wraparound as VACUUM does.
Pg 9.4 changes some of this, so FULL deals with id wraparound too, somewhat.
Also, in Pg prior to 9.0, FULL worked differently.

* See http://blog.endpoint.com/2014/02/dbdpg-utf-8-perl-postgresql.html and
why we really should use DBD::Pg 3.0+ if we have non-ascii chars, it is much
smarter than 2.x, actually paying attention to client encoding.

* See http://blog.endpoint.com/2014/02/mysql-ascii-null-and-data-migration.html .

* See http://www.sqlite.org/lang_savepoint.html ; apparently SQL savepoints
are already a generalization of "transaction" that can be nested and don't
have to require a parent "transaction" to be active; the first "savepoint"
could take that role itself; this is a lot like my design plans actually.

* Things even SQLite can do (as can Postgres) but MySQL can not do:
- subject data definition to transactions
- savepoints
- WITH clause, including recursive
- EXCEPT clause

* Apparently Windows 8 extends the use of shared pages to allow any pages
that just happen to be identical to be shared. It periodically sweeps
through all the memory in the system, identifies pages that have the same
contents, makes them shared, and frees up some physical memory. If one
process should then try to modify the newly shared page, Windows uses COW.
http://arstechnica.com/information-technology/2012/10/better-on-the-inside-under-the-hood-of-windows-8/3/
So my plan to do similar in Muldis D impls (when = is invoked) does not
seem that unprecedented, though in my case its both memory+performance.

* Apparently C# has a "where" construct useable like this (it is lazy):
    var q1 = q0.Where(MoreThanTwenty);
... note that MoreThanTwenty is the name of a predicate function.
Also it has a "select" that works like "map":
    q = xs.Select(x=>1/x);

* See http://queue.acm.org/detail.cfm?id=2611829 - The Curse of the
Excluded Middle.

* See also "Haxe" language/platform.

* See also Test::Database the likes of which I can also support.
Its HISTORY.  http://www.nntp.perl.org/group/perl.qa/2008/10/msg11645.html
Quoting Michael Schwern:
"There's plenty of modules which need a database, and they all have to be
configured differently and they're always a PITA when you first install and
each and every time they upgrade."

* PostgreSQL's model about transaction ids, and that it has special {1..2}
reserved values may be useful in my design.
http://www.depesz.com/2013/12/06/what-does-fix-vacuums-tests-to-see-whether-it-can-update-relfrozenxid-really-mean/

* This is probably useful, concerning timestamps:
http://www.depesz.com/2014/04/04/how-to-deal-with-timestamps/

* NEXT PRIORITY ...
Rename Muldis::D::Dialect::PTMD_STD to Muldis::D::PTMD_STD.
Rename Muldis::D::Dialect::HDMD_Perl[6|5]_STD to remove the ::Dialect.
Split off the Perl STDs from Muldis::D so they're no longer core
but they're still official, and distribute the HDMD_Perl5_STD in the same
distro as the Perl 5 Rosetta Engine API documentation.
(SEE ALSO THE TODO FOR MULDIS::ROSETTA FOR CONTEXT.)
Lets call that documentation distro Muldis::D::Perl5.
Create it like how we created Muldis::D::Manual ... empty then transfer.
The meat module lib/Muldis/D/HDMD_Perl5_STD.pod is renamed as such so it
can also have its version reset to 0 and then the new distro can
advance versions on its own without needing to match or incorporate the
Muldis D versions which would become onerous.  But the language declaration
name would not change, except to add more vnums, eg to have:
[ 'Muldis_D', 'http://muldis.com', '0.149', ['HDMD_Perl5_STD', '0'] ]
Add new directory mod lib/Muldis/D/Perl5.pod like Manual.pod.
All of this likewise for the Perl 6 versions.
THIS GIVES US MORE FLEXIBILITY TO NOT HAVE TO UPDATE THE PERL-STD SPECS ON
THE SAME SCHEDULE AS THE TEXT-STD/REST.

* SAME TIME AS ABOVE ...
Actually, take this opportunity to overhaul the concept of qualified
language names and such.  Promote "dialects" from being subservient to
language into being full languages of their own that are derived from
Muldis D.  That is, say that what was "PTMD_STD" is now simply *the*
concrete syntax of the official "Muldis D" language, and that what was the
Perl dialects are now 2 full languages whose definitions are derived from
the standard text-based one, and have their own versioning.
- So, here are new examples of fully-qualified language declaration names:
    Muldis_D:PT_STD:Unicode(6.2.0,UTF-8,canon):"http://muldis.com":0.149
    ['Muldis_D','HD_Perl5_STD','http://muldis.com','0']
    ['Muldis_D','HD_Perl6_STD','http://muldis.com','0']
- Actually, we are best to split things up so that Muldis D code typically
declares 2 main versions separately, which are the concrete syntax version
and the system catalog / core libraries version.  Also, the Muldis D spec
itself would start using different version numbers for different parts,
so the concrete syntax and system catalog or core libraries would be
differently versioned and would only increment when those parts change.
- The Muldis D core spec could use at least 3-4 version series:
    - The version of the documentation distribution: 0.149 is next.
    - The version of the Core (name subject to change) library, which
    determines the data types and routines intended for users to use.
    - The version of the system catalog schema, that is the version of the
    "real" Muldis D concrete syntax / data structures as seen by code.
    - The version of the PT_STD concrete string syntax that users write in,
    which a parser needs to know but the system catalog doesn't.
    - All of the above would start at version 0 except the first item.
    - The second item Core is what is expected to change the most often,
    while the catalog or string syntax are expected to change less often.
    - The second/Core and third/Catalog might be stay merged into one item
    as they are kind of inter-dependent, but we may review minimizing Core.
- Example of Muldis D code declaration of language:
    Muldis_D:PT_STD:Unicode({1..6.1},UTF-8,canon):"http://muldis.com":{0..42};
    pkg ::= package FooLib:"http://foo.com":0;
    MD ::= using Muldis_D:"http://muldis.com":0;
    Spatial ::= using Muldis_D::Spatial:"http://muldis.com":0;
    Pg ::= using DBMS::Postgres ... emulations of all Pg types/routines/sys-cat/etc
        ... this is defined as a wrapper over Muldis_D/etc, but in a MD
        impl over Postgres, these would likely be implemented natively
        and then Muldis_D::* is implemented as a wrapper over these
    dfoo ::= using DBMS::foo ... emulate other DBMS
    Perl ::= using lang::Perl ... emulate Perl
    lfoo ::= using lang::foo ... emulate foo
    vxyz ::= using VendorXYZ::FooLib:"http://vendorxyz.com":0;
    Lulz ::= using MyAppBar::Lulz:"http://mybusiness.com":0;
    searching [pkg,MD];
    ... then our own module/depot/value/etc content follows ...
- Note, "using" is the C# keyword for this.  Note, "System" is C# core lib.
- Note, "java.lang" is core Java package; it is precedent for "Muldis_D::*".
- Note, a Java "package" is like MD's package/module concept, in that it
can define multiple classes/interfaces etc as per my multiple types/etc.
- Having DBMS::* and lang::* etc makes it easier to build compatibility layers
as eg a SQL parser just has to parse into DBMS::foo stuff and that's it.
    - A DBMS::* package also defines alternatives to Tuple/Relation types
    to represent SQL ROW/TABLE/etc types; these most certainly are all
    Scalar types from Muldis D's perspective and loosely are Tuple/Relation
    wrappers but that extra meta-data is included such as a concept of
    attribute order that is built-in to each value, or of attributes being
    allowed to have non-distinct/anon names, or of having certain 3VL etc.
    - Probably make the "real" names as numbers, like SQL does, and the
    text names are then aliases.
    - Our letter case of package entity names matches the folding rules for
    that DBMS with unquoted identifiers; in Pg they're lower, others upper.
- We use "::" as separators in package/module/extension names because those
are more public namespaces, whereas "." is used for internal namespaces
that subdivide each individual package/etc.  By doing this, if package Foo
exists with internal namespace Bar and routine baz, there won't be any
ambiguity if Foo::Bar comes to exist with a baz() in its root namespace;
they are invoked as Foo.Bar.baz versus Foo::Bar.baz so a clear distinction.
- The package name of the Muldis D language core is simply "Muldis_D"
(keeping it simple/terse); so no reason to explicitly say "Core"/etc.
- Strictly speaking, all other packages don't need to have any particular
hierarchy or be in any particular package namespace, though it would be
good to come up with something early on.
- Note that say a code file that is simply a direct translation of, say,
SQL, may not "using" Muldis_D::* at all, but just the DBMS::* in question.
- In fact, we should emphasize a feature of Muldis D where it is suitable
as a very good alternate syntax for SQL, easier for ORMs to build, etc.
It can exactly repr semantics of each SQL dialect while being cleaner syn.

* Version numbers of anything by authority http://muldis.com are formatted
as strings of 1..N nonnegative integers, where each string element is
separated by a period, and strings sort pairwise one element at a time from
start to end, where each element sorts numerically and shorter strings sort
before longer ones whose leading substring they are equal to.
- By default, version numbers are just a single string element and add more
when they feel they need to; most of the time, just 2 explicit elements are
used, and the initial version of any project tends to be the single zero.

* Make the fully qualified language names declarable by code to be more
flexible than the names declared by the language spec itself, so that the
ones in code can specify multiple language versions that they conform to,
as if the code is declaring that it only uses the parts of the language
spec that are unchanged or that intersect between all the specified
versions.  For example, let one say:
    Muldis_D:PT_STD:"http://muldis.com":{0.112..0.125,0.127..0.136}
... and then any implementation which takes one of those versions will
parse the code according to any of those same that it supports.  The idea
is to make code more easily compatible with a wider range of interpreters,
such as newer ones designed for version 25 who don't explicitly know how
to emulate version 23 or know what its differences are, and are just
trusting the code to be valid for version 25 even if declaring 23; by the
code declaring a range, it is declaring it is willing to take its chances.
- When an implementation provides or emulates multiple of the language
versions that the code declares itself to be valid for, the implementation
is free to treat the code as if it was just any one of those versions, and
which version is chosen is formally undefined; the implementation can just
pick any version that it chooses and the code compiles/runs or not as such.
If the code fails to compile/run as any random version it claims to be
compatible with, the implementation shall *not* try again as another
version in the list as a fallback, probably.
- A multiplicity of stated authorities is also possible.
- A number of consequences still have to be thought out here.
- Specifying a closed range is the code saying it *knows* it is compatible
with all those versions, specifying an open range says take chances or is
not recommended and maybe won't be supported.
- Or, rather than having open ranges, we could stick to just closed but
have both whitelist and blacklist ranges, where we explicitly specify what
versions we *know* either are or are not compatible, and so then any
versions not explicitly named in one of those 2 lists is considered unknown
as to compatibility.
- Similarly, an implementation could be introspectable for the same 2 lists
white and black that it supports for implementing.
- It would be possibly implementation-defined or externally
user-configurable for what happens if the implementation only supports
language versions outside the whitelist including versions outside the
blacklist.  A stricter setting would say reject, permissive may say accept.
- For permissive, its possible that if vnums are fully orderable, then a
nonmatching implemented vnum directly between or just next to versions in
the whitelist could be considered to work, likewise for blacklist
considered not, and between one in each list considered not.  Eg, if the
program code declares {!1..5,6..10} and the implementation just provides
version 12, then it could be considered to work by default.
- In the interest of being nullary, we could permit code not specifying any
version number at all, or authority, same as Perl 6, meaning that the code
does not explicitly consider that it would work or not work with any
specific language/module/etc version at all.  So a strict implementation
would reject all such code out of hand and a permissive one would accept
all such.
- On the other hand, in the interest of preventing bad habits from
starting, where people just leave out the auth/vnum specifiers and then
people seeing that code not realizing that being specific is even possible,
we could say that it is always mandatory to specify at least a
single authority+vnum that the code is known/declared to work with, so that
any implementation has at least some idea to work with as to the likely age
of the code or what implicit expectations it might have.  The writer of the
code had presumably been testing it with *something* before releasing it.
And even neophytes who copy-paste whole code would be getting either
correct behavior or a rejection by an implementation from version diffs.
And savvy people that go around the internet can more easily spot bad or
outdated code examples in tutorials or script archives because they'll just
have an old dependency version specifier and not mention newer versions.

* Consider use of the term "curator"/ed/ion/etc with respect to authorities.

* Lets say that language name declarations are mandatory in code files but
are optional in ad-hoc code read from a shell or fed from a host language,
because in the non-file cases the shell context or host language could
previously/separately set the context for what language is expected.  In
the case of a shell, lets just say that a language name declaration is
written just like a shell statement or command; we know its that kind of
command if it looks like a language declaration, as no normal code would.

* See also Jesse Vincent's "Perl 5.16 and beyond" talk, where Perl 5 core
policy or Jesse's proposals for such are discussed that when you say "use
v5.xy" then no matter what the actual (newer) executing Perl 5 version is,
you have declared that you expect it to behave with the same semantics as
the same version 5.xy that you named in your code.

* See also
http://tech.valgog.com/2012/01/schema-based-versioning-and-deployment.html .

* Note, http://strangelyconsistent.org/blog/dash-n-and-dash-p-part-three
clarifies what "setting" means in Perl 6, and there can be more than one of 
them at a time that are layered; it does not mean "language core"; Perl 6
has the separate CORE concept, so staying with Core seems approp for MD.

* Remove TTM concept/language of "possrep" from Muldis D spec, at least in
the way it is presently used.  Instead, each scalar base type is simply
defined in terms of exactly 1 list of components/attributes, which doesn't
have a name (or corresponds to the empty string).  Also, the idea of
"possible representation" actually applies to the whole language, including
nonscalar types, because the DBMS can still choose any physical
representation it wants; its not like scalars are special there to reserve
the term "possrep".  Additional "possrep" are just syntactic sugar for
wrapper routines or such or pseudovariables.

* Quoth Anthony Clayden: "Specialisation by constraint is the defining
characteristic of the TTM model".

* Don't use the term "scalar" to refer to things that are neither relations
nor tuples as TTM does; conversely don't use a term to refer to things that
are either relations or tuples but not something else; just say "tuples or
relations" when that is meant, which wouldn't be terribly often, and
similarly "values/types that are neither tuples nor relations" when meant.

* Change Array/Set/Bag/Dict/Interval/Maybe/etc so that they are no longer
relation subtypes but rather are disjoint.  Generally eliminate any
system-defined tuple or relation subtypes, leaving all such to users.
This should help eliminate a number of gotchas from Muldis D that could
trip users up, such as users sometimes having to know the names of the
special relation attributes despite those being hidden in practice.
Also we would not confuse semantics; eg, tuples and relations
have predicates but sets/arrays/etc do not.
Emphasize the fact that Arrays are quite different from all the those other
types, in that they have a dense keyspace starting at a fixed point, where
the others are all sparse; eg, never make Array a subtype of Dict or such.
Similarly, consider defining Array directly in terms of List rather than
say in terms of a "scalar" with a Relation component like with Dict/etc,
because List can be used natively; but leave unordered types as over rels.
So Array's like, but disjoint from, String; list of anything versus of int.
The various details need more thought.

* Have the 10 Relation subtypes
"Relation.{K0,K0C0,K0C1,D0,D0C0,D0C1,D1,D1K0,D1C0,D1C1}" where "D" means
"degree", "C" means "cardinality", "K" means "degree of minimal key".
Having D0 implies K0 but K0 does not imply D0; also, K0 bi-implies C0,C1.
And thus we have names for all the relations that can be used unambiguously
without any need to know their attribute names for special purposes.
The D0s are the boolean or identity values for join.
The K0s (and D0s) are the "maybe" types with C0->"nothing", C1->"just";
but there is no single "nothing" value anymore, rather an infinity;
for that matter, redefine the type "Maybe" as an alias for "K0"; despite
the heritage, we may want to ditch the "nothing"/"just" names actually.
The D1s are convenient in that one may reference their attribute
unambiguously without knowing its name.

* All the selector syntax specific to Set/Array/Bag/etc would produce
non-relation values, as if it did, the attr names would be implicit.
But Set-taking functions should also take any D1 relation as an alternate.

* In an attribute list, the first of these is a shorthand for the others,
or specifically for the second, but the third has the same semantics:
    maybe foo : Bar;
    foo : maybe_type { foo : Bar };
    foo : relation_type { foo : Bar; primary_key {} };
... but while the first can only do D1K0, the latter can do K0 in general:
    foo : maybe_type { x : Bar; y : Baz };
    foo : relation_type { x : Bar; y : Baz ; primary_key {} };
So the new maybe is a more workable/flexible way of handling nullability.
Actually, the "maybe foo : Bar" reads interestingly because it might
suggest to some readers that having the "foo" at all is optional;
however the latter thought is not one that we want to encourage per se.
Remember, "maybe" and "set"/etc are now completely disjoint.

* Operators like "//" that deal with maybes need to be closed, where both
arguments must be maybes and the result is a maybe.  This means that the
final/default value in a //-chain must be a just, eg:
  x // y // :@{{3}}
... where each arg can be any zoo relation, and :@{{3}} means :@{{0:3}}.
And so the //-chain is then followed by attr extraction when that's wanted,
eg: "attr(x // y // :@{{3}})" for D1K0s or "(%(x // y // :@{{3}})).foo" or
something for K0s in general where we must name the attr.
Note that a // defined this way can't have an identity value since multiple
values would qualify for the semantics, same as with bitwise-or.
Note that overloading //, eg to work with some "special" value to mean
null, can only accept non-relation args in practice so not to overlap.

* Make a depot/etc have 3 main payload sections (each of them being a
"Database") rather than 2; where we had "catalog" and "data" before, now
add "constants"; names of the 3 subject to change.
- The purpose of "constants" is to be the most effective way to represent
the bodies of non-trivial niladic functions, or the values of non-trivial
expression trees with no free variables.
- The "constants" are semantically part of the code, and changing them is
considered a data-definition rather than a data-manipulation; however they
are stored directly as the values they represent, same as things in "data",
rather than as trees of expression nodes in the catalog.
- Unlike things in "data", you can refer to "constants" directly in the
bodies of function or type definitions.
- Each constant typically lives in its own RVA of the constants database,
or several may be in the same if they have the same structure or type.
- An example usage is that the Unicode character database could be
accessable to Muldis D within a "constants" database of some appropriate
module.  Another example is parser rules defined as relations.  Another
example usage is for the user text of programs, or exception/error
messages, especially multi-lingual ones.
- Where one wishes to comment individual items in a collection constant,
they do so by adding extra RVA attributes for the comment text.
- You do not normally use constants to define enumerated data types that
define new values, such as Boolean etc, which are unions of singleton
types; but you can use constants to define enumerated types that are subset
types of others.  Constants are analagous to values/functions, not types.
- Constants live in the same shared schema-namespaces that are shared by
types/routines/etc and relvars, and all must have distinct names therein.
It is as if the "constants" and "data" databases are superimposed, so no
RVAs in one may have the same names as those in others, but schema/TVAs may
exist in common.
- One always uses function syntax to refer to "constants" RVA/TVA within
routines, as if each one was in fact declared as a niladic function.
- The only way to refer to "constants" as variables in routines, for
dynamic data-definition, is in procedure pseudo-parameters, same as when
referring to the catalog database or "data".
- We would probably have special/shorthand syntax for declaring constants
in code like we have for declaring types or routines, etc; for example:
    FOO ::= constant (...);
    dbname.schemaname.BAR ::= constant (...);
- Similarly, whatever syntax we use for nesting a higher-order function
definition or a type definition in a routine, we use something similar for
nesting a named constant declaration in a routine, in order that it be
treated as a "constants" item rather than an expression tree that it would
otherwise be.
- You can't use "constants" for values that are not deeply-homogeneous,
like something kept in a "Database" must be; for those you have to use
regular expression trees.  But it is expected that in practice any values
that aren't deeply-homogeneous would be transient or generated at least
partly from free variables, so this limitation shouldn't be a problem;
or you should likely be able to refactor the values down into
deeply-homogeneous inputs to a simple regular value expression or niladic
function which then gives the actual desired vaulue.

* CHANGE AFFECTING THE ABOVE ...
Change the "Database" type so its attributes may be "Relation" in general
and don't have to be "DHRelation"; that is, the type of a database relvar
attribute may now be anything up to "Universal".  Or alternately, perhaps
go for some kind of middle ground where we say that every relvar tuple
attribute value must be a Scalar|Tuple|Relation and not some other List.
- Or alternately, the language supports "Universal", but users can request
varying levels of strictness for their databases, such that it should be
very easy for them to declare they only want to allow DHRelations; then
again, the existing facilities already let them ... the declared db type.
- This should simplify Muldis D in some respects, because we would be
removing a language limitation that is more arbitrary than essential.
- Now this change might seem to complicate certain database design or
implementation issues, in that the general case of a relvar TVA/RVA would
no longer be refactorable into a single other relvar per TVA/RVA, if
different relvar tuples might have tup/rel attrs of different headings;
each possible one would then have to become a separate relvar, or otherwise
such refactor just wouldn't be possible.
- However, in practice we already face this problem by our TTM-blessed
support for scalar union types in relvar attributes, generalized to
"Scalar" typed relvar attributes, should we, say, wish to unpack such
scalar values into relvars, or implement as such behind the scenes in some
engine that just likes to have simple homogeneous scalar relvar attributes.
And so, allowing it for TVAs/RVAs just makes things more consistent.
- I also don't see any logical issues with this support, in general.
- This change also means that a "constants" db *doesn't* have to be DH.
- In theory we could also take a next step and have Muldis D support
"databases" that aren't relational at all, as an option, but rather let the
"database" value be of any type at all.
- Or to keep things more manageable, we still require that a "database" is
some kind of "Tuple", but then it is no further restricted than that.
That is, we require "Tuple" so it can be overloaded as a namespace for
types and routines etc, including the restriction on when type/routine
namespaces and attributes may be equal or not, if the current language
grammar even requires them to align, but we will assume that it does.
- So then, the system-defined type "Database" simply goes away, as "the
database" is just a "Tuple", but we can make it very easy for a user to
define a "Relational_Database"/etc subtype with whatever level of
restrictiveness they desire.
- And then, one can also more directly represent a SQL database in Muldis D
(or some other non-relational db/storage du jour), not just by using the
appropriate packages that define DBMS-specific types/routines, but by also
declaring that "the database" is of some DBMS-specific/allowed type, such
as, that all database tuple attrs are "Table" typed rather than "Relation".

* Make Muldis D self-hosted.  Have it support parsing PTMD_STD code using a
read-only operator into trees of scalars that are the new native Muldis D
code, which the older system catalog is just views on.  Make it support
writing the full Muldis D compiler in Muldis D, that can eg generate Perl
code, and then the Muldis D compiler can compile itself.  That is, the
reference implementation would be compiled to Perl, and that Perl code is
capable of translating equivalent Muldis D code into that same Perl code.

* Ooh! Oooooh! The trees of scalars form of Muldis D code might just then
give us the option of more comprehensive higher order functions, where you
actually pass the function definition as the argument, rather than its
name, as you do now, though the latter would still be perhaps easier to
implement on more foundations. We'll see if that's necessary or works.
That would certainly be more of the true anonymous function nature.

* Note that the typical action of Muldis D implementations could be called
"transcompiler" or "source-to-source compiler" because they translate
between high-level languages rather than directly to machine code.

* The new system catalog would have several main namespaces where each
comprises a different view of the same package code.
- The primary view, maybe call it "trees", would present each complete
package/module/extension as a single scalar value that is a concrete syntax
tree package node, whose attributes contain the rest of the tree defining
the whole package.  These trees are composed of the nodes that are now the
native Muldis D code format, which PT_STD very closely approximates, and
the HD_Perl*_STD slightly less closely approximate.
Preserves details such as: whether child expressions were written nested
under parents without explicit node names vs nested with explicit names vs
off-side with explicit names, and
all code comments that are explicitly bound to names or parts of code like
named declared entities, and the visual sequence of code elements in the
string source, and whether operators are invoked prefix or infix, etc.
Does not preserve details like the actual whitespace in the code, or the
actual string escape sequences, or whether identifiers were quoted or not,
or the exact format of the numeric literals, and so on.
Does not explicitly hold gen names for nodes that were anon to programmer.
- Another view, maybe call it "relations", can be loosely what the
current/old system catalog is, having all of the same information as
"trees" but all broken out into a flat namespace, like the current/old but
that all the RVAs/etc are basically ungrouped or split off into separate
relvars.  Or maybe it would be partially flattened and partially not,
whatever makes for the easiest use by people, or there could be multiple
sub-views showing varying degrees of being flattened.
For various reasons it may be wise for "relations" and "trees" to
be information-equivalent, so each is fully updateable with changes
propagating to the other deterministically and losslessly.
- As "relations" has explicit names for all expr/etc nodes while "trees"
only does when the programmer explicitly provided them, names must be
generated when deriving "relations" from "trees" and stripped the other
way; the names must be generated deterministically, and a constraint must
exist on "relations" so explicitly setting them differently is disallowed
while the "generated name" attr is also true on that node; expect that said
gen name is probably something like catenating the names of the ancestor
nodes in the expr/etc trees starting with the nearest non-generated one;
the generated names could take the form of a chain of integers, one integer
per ancestor level, where the integer is the ordinal position of each node
beneath its parent, starting at zero.
- Another view, which is not information-equivalent to the others, would
have further derived information, say more resembling the SQL system
catalog, say showing in one place what all the actual attributes or known
keys constraints of a relvar are, regardless of whether they are defined
using special syntax or as value expressions.
This view would be writeable, but as it is a dependent on the others,
changes to it would propagate in certain ways, such as how changes to a
lowercase attribute would propagate to a mixed-case determinant one.
- The above views, or at least the 2 information-equivalent primary ones,
would be provided by the language core; the last one might be an extension.
- To clarify, views are allowed between modules, so eg one can define
another kind of system catalog, but only between modules that are loaded as
extensions rather than depots, I think.

* An EXTENSION module, eg Muldis_D::PT_STD, is what provides the routines
to translate PT_STD code strings into Muldis D's native dialect, the trees
of scalar nodes that the "trees" catalog view uses, and said extension also
provides the reverse translator; working with PT_STD code strings is *not*
part of the Muldis_D core module, and such would only be needed in a
Muldis D program implementing a REPL or a compiler or one that uses PT_STD
as its disk storage format.
- This extension would also define another set of syntax node types which
holds a lot more concrete syntax details than the native node trees, such
as the actual whitespace used between things or the actual format of
identifiers (bareword vs quoted) or any extra/explicit nesting parenthesis
and so on, such that the exact same PT_STD
code string could be reproduced from it; mainly these would just be used
internally to the extension, except say when someone wanted to extend it.
- The PT_STD->native translator function would probably have 3 main other
functions that it invokes serially, where one is a tokenizer that just
converts the PT_STD string into a flat array of tokens, where each token
may be represented by some token-type-specific scalar value rather than
being a Text value, so eg each end of a delimiter pair and whitespace hunk
would be a separate token value here, and the second function takes this
token array and converts it to a tree, eg combining delimiter pairs into a
single value with children from 2 without; the third converts this full
concrete tree to the quasi-concrete tree of the native Muldis_D.
- It is expected that some level of Muldis_D::PT_STD is what deals with
unescaping string literals or compat-normalizing Unicode etc or otherwise
following the wishes of concrete syntax directives.
- Similarly, the extension would be what determines whether a string is
valid PT_STD, including that it provides a "is this PT_STD" bool func.
- Similarly, Muldis_D::PT_STD would provide config options for the
translater to PT_STD from native, say on what num format opts to prefer
or what length to wrap lines etc.
- To help specify a pure functional parser that is both resistant to having
too-deep levels of recursion (say, a process defined on head+rest), we
should utilize some of Muldis D's more unique features, mainly the
relational operators, and define the parser largely in terms of say
relational join and such, so it is more conceptually parallel than
recursive (or iterative).
- Have a function that derives an Array-of-Codepoint (or similar relation)
from a String, then use it on the PT_STD source string as the first step,
and then say do tokenizing in terms of joining that relation against a
static relation having say 1 tuple per distinct character in the text
repertoire, mapping for example traits like is-whitespace or is-symbol or
is-alphanum or is-string-delimiter or is-escape-char (\), so say the result
of the join is the indexes into the original string of where tokens are.
Filtering out say escaped chars from unescaped ones is easy; find all
occurrences of the char and then set-minus those with a "\" just before.
It is useful to do edge detection early by joining a folded (to char kind)
string to itself offset by one index position and seeing where the pairwise
characters are not of the same character kind.
The static relation of characters may be best stored in code in a compacted
range-defined form, and if expanding is needed, do it when needed;
then we may only need to store a few dozen tuples rather than thousands.
- This same design would scale transparently to different repertoires,
eg ASCII vs Unicode; just the static tables of characters are larger.
- The very first step in parsing PT_STD:Unicode(...) is to bring the source
Text to a normal form, either "canon" or "compat" (with semantics of NFD or
NFKD in the spec) and then the PT_STD parser proper then works with that
Unicode normal form as its input.
- We would have a whitelist of codepoints that may appear outside of a
quoted string in the source, and all others are illegal outside, for
security purposes (lookalikes) if nothing else; it is applied post-normal.
This makes it similar to pigeonhole charcters into symbol vs alpha for
example, where that distinction mainly just matters for barewords.
- We have to figure out how best t work w Unicode combin chars for parsing.
- Note, according to http://en.wikipedia.org/wiki/Unicode_symbols, Unicode
has a main distinction between "scripts" and "symbols".

* Thanks to syntax/package separation, we have a nice semi-orthogonal
situation where PT_STD syntax can be used to write Muldis D or SQL,
and SQL syntax can likewise be used to write both, in theory.
- That is, the package determines the "semantics", the grammar the syntax.

* It remains to be specified how to support code expecting different
versions of Muldis D or other modules, such as where to say no and where to
emulate and at what level to emulate.  Perl 6's solution may be a guide.
We might consider multiple packages as a solution where different packages
provide different versions of the Muldis D API, and they can be used at
once, similar to having packages provide the APIs of SQL / other languages,
so that the parsers eg for PT_STD don't have to do it themselves for
modules but rather just for concrete syntax vers.  Still open to debate.
- The best way to do this emulation cleanly, for package/module versions,
is that in the general case we consider every name+auth+vnum permutation
to be a distinct module, and that permutation is the actual name of the
module, so code expecting/using different module versions is using
different names to refer to its entities.
- It is mandatory for a package/etc to, for each of its explicit module
dependencies, to also declare a package-local alias for that dependency,
by which name it must always refer to the package or its entities.
- The effect that this has is like that of the global params of recipe
routines; there is no longer a global namespace of used packages in the
DBMS, but that rather each package's code only ever directly sees within
the same package, and any used packages' contents are aliased to namespaces
within the using package; the language core pkg is treated the same way.
- An implementation may provide packages for different versions of the same
base package at once.  When a package's "using" statement indicates that a
range of versions are suitable, the DBMS implementation would just pick a
single one of those for which it actually has that dependency package
version and associate the user's local package alias to that specific
version longer name.  Since a "using" specifying multiple versions means
any of those should have the same semantics for features used, the DBMS is
also free to remap at runtime which of several satisfying versions it has
to the alias, so that say if multiple dependent modules specify overlapping
but unlike dependency versions, the DBMS can choose to use the same version
it has for both in the intersection of those ranges, without breakage.
A package's code should not generally need to know which specific
dependency version it actually got or has, though there might be rationale.
- The package-specified alias specifier might conceivably be more generic
than just a Name; it might be a path in the using package's namespace into
which to "mount" the other package's namespace.
- Depending on how the aliasing works, conceivably if package Y explicitly
uses X and Z explicitly uses just Y, Z can also access X's namespace within
Y's namespace that it sees.  ACTUALLY, NO, that's a bad idea; Z should not
see any part of Z by way of Y but for Y's explicit synonyms of things of X.
- Regardless, when dealing with higher-order functions and such, those
functions are a closure of sorts wherein all references to package entities
in the function code are resolved relative to the packages where the
functions are declared, not relative to where they are actually invoked.
- We also need to revisit matters of depot mounting and such, and see if
the aliases we declare there and for packages can be generalized together.
- Perhaps from each package's perspective, the entity namespace is this ...
    /local/*  - current package's namespaces/entities
    /used/<alias>/*  - each used package's (or core's) namespaces/entities
... those being the absolute paths.
- Or perhaps better yet, if each package is also required to declare an
internal alias to refer to itself with, the above can be flat, just:
    /<alias>/*  - current or each used or core pkg's namespaces/entities
- Separately, each package needs to declare an explicit ordered list of
search paths that it will use internally by its own code to resolve
unqualified entity references in its own code.  Typically the first 2 items
in the list will be the package's own self and then the core language
package.  The list can be either empty or include all the used packages;
searching will only look in this list, and one can't use search/unqualified
syntax to reference anything outside this list, just relative or absolute.
- Example:
    Muldis_D:PT_STD:ASCII:"http://muldis.com":0;
    package pkg ::= FooLib:"http://foo.com":0;
    using MD ::= Muldis_D:"http://muldis.com":0;
    using Pg ::= DBMS::Postgres ...
    using Spatial ::= Muldis_D::Spatial:"http://muldis.com":0;
    searching [pkg,MD];
... uses alias "pkg" for current package, "MD" for core, and so on.
- Making the explicit search path list mandatory in order to get
unqualified referencing is important because then neophytes can't just
copy-paste parts of code from random online tutorials or script archives
without being explicit about resolving any references therein, which can
trip up people where the same code may resolve to different operator calls
depending on what modules are used.
- Perhaps having just the language core in the search path list and not
even the current package is best in some circumstances.
- Note http://www.postgresql.org/docs/current/static/ddl-schemas.html#DDL-SCHEMAS-PATH
which has a "search_path" setting that works similar to this.
- But the aforementioned is for schemas; therefore Muldis D may be best to
offer a more finely-grained option, where Muldis D's search path takes a
list of name-chains rather than names.  This has an advantage of letting
one select just parts of a package to search, or the order within a package
to search, etc.  Each name-chain in a search-path list can be either
absolute or relative, the latter being resolved relative to each individual
reference; or better yet, make each search-path item absolute only and that
the language separately has the option to search relative to the
referencing entity first.  This needs more thought.
- For implementation, we are probably best to preindex all the packages,
where all possible unqualified or semiqualified namechains are mapped to a
list of all fully qualified chains they match the endings of, so no actual
"search" is necessary; we just have to evaluate on arguments later.

* We also need some distinct syntax for defining that a package definition
is split into multiple pieces for storage, which for source code typically
means one disk file per piece, but they are logically still one package.

* Consider making the "lib" or "cat" or "data" portions of namechains
optional in contexts where they would always be the same, especially "lib",
so for example one can just say "nlx.myfunc" rather than "nlx.lib.myfunc";
this would be loosely similar to the elimination of "lex".

* For that matter, consider restructuring the namespace tree in Basics.pod
so the lib|cat|data|etc are the namechain first element, with fed|nlx|etc
the second element; and sys doesn't have lib|cat|data already, or in fact
that would all be grouped under "lib" arguably.

* For that matter, lets just eliminate the special namespace prefixes
entirely, even as an option, because typically certain contexts only allow
certain kinds of things anyway, such as just lib or data, and so one should
never have to or be able to write chain elements that mean anything other
than their literal selves.
- Rewrite ENTITY NAMES in Basics.pod to get rid of the formally structured
shared namespace as it currently is, and instead have that pod divided into
context-based sections, saying for example "in this lexical context a
namechain means this" and "in this nonlexical context it means this".
- Where one needs to explicitly use an absolute path vs an explicit
relative path vs a searching relative path, we can provide a new data type
which wraps the basic name chain of old to provide this.
For examples (new arrayish structure <-- old way of stating):
  ['abs',['mydb','myschema','myfoo']] <-- fed.{lib|data}.mydb.myschema.myfoo
  ['rel',2,['myschema','myfoo'] <-- nlx.par.par.{lib|data}.myschema.myfoo
  ['rel',0,['mybar']] <-- nlx.{lib|data}.mybar
  ['shp',['myschema','myfoo'] <-- shp.{lib|data}.myschema.myfoo
The above could possibly be written like this:
  .$mydb.myschema.myfoo or abs$mydb.myschema.myfoo
  2$myschema.myfoo or rel$2$myschema.myfoo
  0$mybar or rel$0$mybar
  myschema.myfoo or shp$myschema.myfoo
I'm still missing an example when one wants to say "sys", maybe, though
both "abs" and "shp" kind of apply there but kind of not.
Note that in the above it is mandatory for there to be no whitespace
around the "$" ... the formatting is like with "#" in numeric literals.
- Now while there are {$:,%:,@:} prefixes for {S,T,R} value literals, and
the colon-less {%,@} mean cast between tuple and relation,
the colon-less $ doesn't make sense for a similar purpose,
so use $ for name literals, that is, "$foo" means "Name:foo",
so one can write projection like "r keep {$foo,$bar}" not "r{foo,bar}",
and then for example "$<>foo" means "NameChain:foo" (and "$<>" the emp nc).
- Also add the ability to use namechains in declarations too, which is
essentially a shorthand; for example, this:
  foo.bar ::= function ...
... is logically equivalent to:
  foo ::= schema {
    bar ::= function ...
  }
Now technically this new way only lets one declare a nonempty schema but
to make an empty one one can still always say:
  foo ::= schema;
- The scalar type name stored inside a ScalarWP is always an 'abs' one,
or some separate canonical ns that takes into account type name aliasing.
- The scalar type name of a scalar value actually stored in a depot
can/should be exactly what the user wrote as per source code; it only is
resolved to some global namespace when compiled from the system catalog;
this is visible to the user of course in the low-level type system when
looking at scalar values, but not in the system catalog / source code.

* To keep things simple, the core Muldis D language does *not* for the most
part have any Unicode knowledge or knowledge of any character repertoires
beyond what common legacy 8-bit encodings handle; extensions are where
Unicode/etc goes because it is sooo complicated and large.
- The Muldis D core still fundamentally considers String/Name/Text to just
be a sequence of "big" integers, and so still supports Unicode etc, but
what the core lacks is any knowledge say about what Unicode characters are
letters vs symbols vs whitespace etc or base vs combining or normal forms
or canon vs compatibility or case folding rules and collations so on.
- The Muldis D core has the "Text" type which just has the integer string
possrep, and it has proper subtypes for ASCII,Latin1,etc (maybe EBCDIC?)
where each adds a possrep.  The Muldis D core knows a small amount of
Unicode, but just the subsets of its repertoire and character codes/names
for characters in Latin1/etc, such that the base number string possrep
still is valid Unicode character numbers for those characters.
- An extension, say Muldis_D::Unicode, adds the complexity of knowledge for
the rest of Unicode, including the NFD and NFKD Text subtypes/possreps etc.
- The Muldis_D::PT_STD string code parser also comes in 2 or more versions,
where said base version only can work with source with th same simple/8-bit
knowledge as the Muldis D core, and you need another version say
Muldis_D::PT_STD::Unicode to handle source using the wider repertoire,
at the very least so it can parse code properly where the parsing semantics
depends on the notion of what characters are letters or symbols or ws
and on canonical vs compatibility notions of equivalence.
- The Muldis_D declaration at the top of a code file also indicates what
character repertoire or encoding or etc the file is written in (which may
even include the "auth" part of the name), so it appears almost-first,
say like this:
    Muldis_D:PT_STD:ASCII:"http://muldis.com":0;
    Muldis_D:PT_STD:Unicode({1..6.1},UTF-8,canon):"http://muldis.com":{0..42};
    Muldis_D:PT_STD:Unicode(6.0,UTF-8,compat):"http://muldis.com":0;
- There may be further variants for diff versions of Unicode standard.
- Muldis_D::Unicode also adds the synonyms for core routines/etc that have
Unicode character names, while the core just has the Latin1 char names.
- There is still the question about avoiding combinatorial explosions about
lots of other modules that want Unicode op names but that should also
degrade gracefully for implementations lacking them.  Probably one easy way
out is that these can just declare these with declared delimited entity
names using numeric escape sequences rather than literal Unicode chars.
- We should be using subtype polymorphism to our advantage.  The Muldis_D
core package should declare a mixin type "SourceCode" or several such like
"PackageSourceCode" or "ValueSourceCode" etc and then declare a virtual
parser function whose input type is "SourceCode" and whose result type is a
node of the native Muldis D language.  Then Muldis_D::PT_STD would declare
a subtype of Text that composes the mixins say "PT_STD_SourceCode_ASCII" as
well as a function implementing said virtual parser which takes that ASCII
type as input, while Muldis_D::PT_STD::Unicode does a corresponding thing
but for a different subtype of Text as appropriate.  These Text subtypes
would have their defining constraint as, Text value that starts with the
string "Muldis_D:PT_STD:ASCII:" and such.  And so the logic for what parser
to select in a generic case just comes down to regular multi-dispatch based
on analysis of the Text values for characteristics specific to a language,
and it is easy to auto-extend the system with support for new languages,
simply by somewhere "using" a package that decl an impl of the virt func;
but a main importance is that the Unicode vs not thing is no longer a
complicating factor for users.

* Here is the main program of a possible Muldis D cross-compiler to Perl 5
written in Muldis D; it reads PT_STD Muldis D code from STDIN and writes
equivalent Perl 5 code to STDOUT.  One of the first goals of implementing
Muldis D should be to empower this being able to compile itself. 
Presumably all of its dependencies would start out as shims that are
written directly in Perl, which are gradually converted.  Of course, the
following would initially also have to be written directly in Perl too, to
bootstrap. This is a static compiler, which by default would succeed in
producing valid Perl 5 code just as long as the input Muldis D code is
syntactically correct (and defines a Muldis D package), in which case its
output would compile successfully in Perl 5.  Generally all errors which
would not be caught in that process are of the nature of invoking or
referencing some entity that doesn't exist or is of a mismatched type etc,
and then by default those would produce errors when running the Perl code.

    Muldis_D:PT_STD:ASCII:"http://muldis.com":0;

    package perl5_from_mdpt ::= perl5_from_mdpt:"http://example.com":0
    {
        using Muldis_D         ::= Muldis_D:"http://muldis.com":0;
        using Muldis_D::PT_STD ::= Muldis_D::PT_STD:"http://muldis.com":0;
        using Muldis_D::Perl5  ::= Muldis_D::Perl5:"http://muldis.com":0;

        searching [Muldis_D,Muldis_D::PT_STD,Muldis_D::Perl5,perl5_from_mdpt];

        bootstrap ::= stimulus_response_rule
            when loaded invoke main;

    /*************************************************************************/

    main ::= procedure () :
    [
        /* Read the PT_STD source code from STDIN until end-of-file.
           Today we'll just assume that it defines a package. */
        var input_source : Text;
        read_Text_file( &input_source );

        /* Validate the input source code and exit if bad PT_STD syntax or
           it doesn't define a package. */
        if not (input_source isa $PT_STD_Package_Source_Code_ASCII) then
        [
            write_STDERR_Text_line( 'Sorry, that source code has a syntax error.' );
            leave;
        ];

        /* Parse the input source code into a Package-defining AST value.
           This AST is the "native source code" of Muldis D. */
        var package_AST : Package;
        package_AST := Package_from_Source_Code( input_source );

        /* Compile the Package-defining AST into Perl 5 source code which
           when run has the same semantics. */
        var output_Perl : Text;
        output_Perl := Perl5_from_Package( package_AST );

        /* Write that Perl to STDOUT. */
        write_Text_file( output_Perl );
    ];

    /*************************************************************************/

    };
    /* package perl5_from_mdpt */

* To keep things simpler, don't have a distinct "value" parser node but
rather just use "expr" instead; when we only want a "value" we can then
either do further constraints on the parse tree from using "expr" or do
constant folding or both as applicable.

* A (Unicode-savvy) parser should treat Greek/etc letters in the same way
as Latin letters, as to where they may appear as barewords and how they are
interpreted, so eg "" is parsed the same as "abcd", so these are all
var or foo() and not prefix/infix when barewords.
This should work well with how Greek is commonly used in maths.

* I should really be exploiting the variety of mature parser generators out
there to do the hard work for me, certainly in development, but also to
help any Muldis D implementations.
See http://en.wikipedia.org/wiki/Comparison_of_parser_generators for lists.
Dave Voorhis of TTM forum tersely recommends ANTLR (http://www.antlr.org/).
David Barrett-Lennard of TTM forum recommends Coco/R.
Quoth DBL: "Coco/R validates an LL(1) EBNF quite well, but uses a non-ISO
syntax (which seems to resemble Wirth syntax notation). Section 3.6 of the
Coco/R user manual discusses the errors it is able to identity."
The comparison url suggests I should try ANTLR first, as it seems to
generate parsers in more of the languages I'd be interested in, esp Perl.
Including: C, C++, C#, Java, JavaScript, Objective-C, Perl, Python, Ruby.
Or the ANTLR homepage actually says Perl support is at early prototype.
Also ANTLR is BSD, runs on the JVM, takes EBNF as input, generates lexers,
and it has an IDE, and its algorithm is LL(*).
See http://www.antlr.org/grammar/list for various already written language
grammars, including ISO SQL 2003, other dialects;
I should probably use the PHP grammar example to go by for mine.
http://www.antlr.org/wiki/display/ANTLR3/Quick+Starter+on+Parser+Grammars+-+No+Past+Experience+Required\
has the tutorial for the flavor of EBNF that ANTLR uses.

* Note http://blog.endpoint.com/2011/12/sanitizing-supposed-utf-8-data.html
the Perl modules like IsUTF8, Encoding::FixLatin, Search::Tools::UTF8,
Encode::Detect, Unicode::Tussle.

* TODO: Declare explicit subtypes of String in core:
String.U1,String.U7,String.U8,String.U32,String.U64
so String values can be explicitly marked as such and one can know quickly
whether the String is known to be just bits, ASCII chars, octets, etc
and so treat it more efficiently rather than defaulting to "big" semantics.
The U8 and U32 versions correspond to Perl 5's strings with isutf8 off/on.

* Change all the routines that take "Set of Name"/etc as arguments so they
instead take empty/no-tuple relations as arguments instead, such that the
headings of these relations convey the same set of Name.
- They're both relations anyway, but using headings for this is more
efficient, and make more semantic sense for such as projection or ungroup,
such as to say, "make the result look like this".
- Also make the last part of a relation literal optional for empty
relations; eg, so one can just say "@:{foo,bar,baz}" without the ":{}".
- So eg the prior example replaces "{$foo,$bar,$baz}" as project/etc arg.
- Also, the arg to rename can be a tuple rather than a binary relation.
- So eg one can say "%:{x:y,a:b}" rather than "@:[bef,aft]:{[x,y],[a,b]}".
- But we still need an alternative when we want to define attribute lists
dynamically at runtime; so we prob want "Set of Name"<->"emp rel" mappers.

* The generic reduce op (and meta) should be virtual/overloaded, so syntax
is the same whether the inputs are an Array or Set or Bag etc.

* POSSIBLE PARADIGM SHIFT!
Consider that most of the DB programming that is done on the internet is
extremely trivial, and developers are currently doing it using SQL plus
some other language such as PHP/Perl/Ruby/etc.
- So how much would these developers save if they had a simpler way?
- This could be a very interesting and strong business case for Muldis D.
- While I am keeping my focus on doing databases well as the priority, I
can see the use case also that people may actually want to write simple
apps and have them written entirely in one easy to use language, where say
the web app code and the database code are seamlessly integrated, rather
than the SQL-foo impedance mismatch.  So maybe for the many common simple
web/etc apps, one can just use Muldis D for the entire app, so the likes of
Perl/PHP/Ruby/etc are just cut out of the picture.
- While I designed Muldis D to be possible to write a full app in, I
assumed that most people wouldn't do this and would combine with another
language like Perl/Ruby/etc.  But maybe I assumed wrong.
- Maybe for the many very simple apps, using Muldis D for the whole thing
actually *is* the preferential option.
- I think I'm going to have to start pushing this angle more.  Don't just
compete with SQL, but also work to eat PHP's lunch, without doing PHP's
numerous lameness (including baking the kitchen sink into the language
itself).  So push that I make simple apps simpler, but without losing
actual power or flexibility.
- Of course, Muldis D would absolutely not have web stuff in its core, so
an extension would be needed for the web stuff.

* BUT SEE ALSO
http://www.infoworld.com/d/application-development/introducing-opa-web-dev-language-rule-them-all-172060
as it seems that Opa makes many of the same claims as the above paragraph,
such as one language to do the database and server and client side web,
where the DBMS and web server are built-in.  However, Opa's database is
apparently hierarchical, and it has 1 implementation, written in Ocaml.
A commentor says its like ASP.NET WebForms, that being bad.
Fortunately then that Muldis D doesn't actually concern itself with
generating HTML; Muldis D is only meant to replace the SQL+{PHP/Perl/etc}
pair and not also HTML/etc.
Actually, http://opalang.org/ shows examples that look like PHP when that
is mixed into HTML, and Opa calls itself "the cloud language"; different goals really.
See also http://www.infoworld.com/d/application-development/10-programming-languages-could-shake-it-181548
for multiple language mentions.

* Refactor, and establish as distinct concepts, Muldis D stores that are
code-only, which we will call "modules", versus those that are primarily
for data, which we will call "depots".
- The "modules" are where any code that should be shared by multiple
databases would live, and include all system-defined code plus all
user-defined code that is packaged and useable like system-defined code.
- The "modules" represent all functionality that is either traditionally
built-in to a DBMS, whether considered part of the Muldis D core or
implementation-specific, or is providable by third-party exten (including
Postgres' "extension" concept) or is conceivably low-level enough that it
would likely be implemented at least partially in a manner external to the
Muldis D environment so its internals aren't visible to the user.
- The "modules" *can* be an external declared dependency of a depot (and
of each other), that is, something which must be present in order to
interpret/use the depot, so like the system itself, that is ostensibly
this is an exception to the rule about depots must be self-contained.
- The modules are written physically differently on disk, at least with a
different primary keyword, so they can't accidentally be used as depots.
- The modules are always readonly to the DBMS.
- Modules are introspectable by system catalog but that is readonly.
- Modules are all individually versioned as per Muldis D itself and all
entities using such must declare the dependency using full version names,
so to ensure the dependent, especially a depot depending on it to be
understood, is interpreted correctly.  So when a depot or module is
declaring its language version, it is declaring a list of all determinant
modules with the same level of detail.
- The "depots" are the only stores that are possibly writable by the DBMS,
when mounted as such, and are what are normally considered "the database",
and are where all user data lives.  These can also contain code and often
do, and only code living here can be updated at runtime.
- In theory or practice a depot could be entirely devoid of code and just
contain data, in which case unless its declared database type is "Database"
it *must* depend on a (readonly) module to define its constraints/etc.
A big advantage of that is to support the common paradigm where data and
code *are* separate, which is frequently the case.  Even more so when
implementing Muldis D over simple data stores without the native concept
of stored user code.
- Maybe we should consider separate code and data files to be the
recommended default way of doing things, and that putting code in data
files be the less common option, so more like typical programming and
further distanced from the paradigm that Muldis D has long had.
If nothing else it would probably be simpler to start implementing Muldis D
if we assume that is the normal way of doing things.
- Ostensibly a primary difference between a module and a depot is that a
module is written as a plain text code file in exactly the same way that
programmers normally write code such as Perl or C or whatever, done
externally to the DBMS in a text editor with direct access to the
filesystem, and then this is considered static in running programs, while a
depot is read and edited from within the DBMS at runtime, using
data-definition statements.
- The point of this new module emphasis is to reduce the problem of having
to duplicate or alias type definitions and such to make multiple depots
work together; for example, often the types shared by a database and an
application might be in a module so each of the latter doesn't need their
own copies.
- Muldis D will provide 2 completely distinct namespaces for modules and
depots, where routines/types in modules are referenced in the manners
typical for programming languages, often fully-qualified, though I suppose
modules, depots can optionally choose to create synonyms within themselves
for such things outside themselves (only to modules) for brevity.
- Modules always declare their own namespaces as per typical programming
languages and they do not vary by or can be controlled by DBMS users.  So
such naming will need to be managed by external convention such as that
the modules declare their own versions with distinct base names that
indicate their namespace, ala Perl modules.  Only depots are under the
user's control for what base namespace they mount into.
- Modules would still support "data" sections, but these would be readonly
and intended for static data resources of nontrivial size, which would be
more efficiently maintained that way than in the "catalog", theoretically.
- For various reasons, including security, with searched namechain
invocations, modules are always searched before depots.
- Lets say for good compromise on various things that various shorthand
syntaxes for operators, such as using non-foo() notation, are only
available for invoking those in modules maybe and that otherwise there is a
different qualification for module and depot thing invocation.  Maybe.
- For data sections of both modules and depots, we could consider something
like an "inclusion node", which may appear anywhere a value node may appear
and pulls in that value from an external source, typically another file,
and then the semantics are as if it were defined inline instead.  This is
kind of like C's include or maybe PHP's include or something or other.
An inclusion node would be formatted like a tuple literal and its details
would be implementation-specific, like specifiers for depot mounts etc, but
in the default case would probably just point to a file in the filesystem.

* Add core support for an analogy to SQL TABLESPACES, with that support
somewhat resembling the depots concept in form in that it has
implementation-specific parameters such as filenames/paths/etc.  The idea
is users can help optimize some things say by storing different parts of a
depot in different file layouts as would be more optimal for that section
of the depot, eg code vs data, and particularly insert-only data versus
frequently updated or deleted data.  But it is important to note that the
depot is still generally the scope of self-definedness or ACID, and so only
DBMSs that support this across multiple tablespaces may use the Muldis D
analogy to said, and our "tablespace" would map directly to theirs.
- With this concept supported, we theoretically could relax certain
requirements that "temporary relvars" need to be defined by existing in
"temporary depots".  Or caches.  This matter requires more thinking.
In fact, multiple DBMSs already seem to use tablespaces for temp stuff,
including Postgres and Oracle.
- We might repurpose the term "subdepot" for the tablespace analogy, or
call it "storage space" or "storage pool" or "pool" etc.
- See also the TTM list thread circa 2011 Nov 16, and "Fragmentation" of
http://www.softwaregems.com.au/Documents/Sybase%20GEM%20Documents/ .
- Note that Postgres tablespaces are actually not disposable; that is,
losing one is significant damage to the whole database that takes some
non-clean effort to recover from; so just saying you will put transient
data in a tablespace on unreliable storage won't actually work
(clearly this is a missing feature in Postgres).

* NEXT PRIORITY...
Rework routine headings/definitions, especially functions:
1.  Remove native concept of optional parameters; this lack can be worked
around in various ways such as with wrappers or multiple routine versions
or polymorphism or currying or system design that emphasizes greater
specialization or fewer parameters.
2.  Make public routine parameter aliases a wrapper thing.  Each routine
has exactly 1 native name for each of its parameters, which is the only
name shared by the routine's internals and its users, and both sides have
their own independent alias namespaces.  For any function that explicitly
declares itself to be symmetric or commutative or associative or whatever
in its routine heading, it *must* use the positional names "0","1",etc as
its native parameter names.  Or for that matter, we could say that in
general any routine intended to be used infix must have exactly 2
parameters, with positional names, and any with more than 2 would not be
used infix.  The routine-internal alias namespace is simply ordinary
expression node aliasing, such as [numerator ::= "0"], or that's what it
is fundamentally, anyway, though syntactic sugar may be provided for this
kind of aliasing.  The routine-external aliasing would involve auto-gen
wrappers, in a manner of speaking, that are like the tuple RENAME operator,
or certainly the semantics are the same.  There would also be syntactic
sugar provided for defining sets of external parameter aliases along with
a routine definition.  To be clear, while synonym aliases for whole
routines don't add routine definitions, parameter aliases do add routines.
3.  Change the normal format for tuple-type and relation-type to look just
like a routine definition, sort of, in that the attribute list is declared
with the same appearance as a parameter list.
For example:
    Dict ::= relation-type (word : Text, definition : Text) key (word)
        where (<boolean-valued-expr>);
4.  Make all functions without exception have exactly 1 actually defined
parameter, which is a tuple-typed positional parameter, so that the
functions' conceptually defined named parameters are actually attributes of
this tuple.  And so all functions will take 1 argument and produce 1
result.  The result type does *not* have to be a tuple (this is important)
and is typically the normal result type the function otherwise has.  We now
will always use a tuple type definition to define a function's fundamental
named parameter list, meaning its parameter names and their declared types.
But an added benefit of the change is that we can define what
*combinations* of named arguments are valid for the function in order for
the function to produce a normal result; a predicate or contract for the
combined argument list is now defined quasi-declaratively as a
boolean-resulting expression in the tuple type constraint.
Theoretically, the parameters' tuple type can now just define all the
allowed public-side aliases itself, or any other variations of parameters
including optional or what-have-you.  But in practice for most functions
we will still just want exactly 1 variant defined, so that the body of the
function only has to deal with 1 variant.
In practice there will exist an automatic RENAME-alike operation that maps
the tuple of a caller with 1 set of argument names to the names that the
called function actually wants, and this would probably be auto-generated
per spot of invoking code for efficiency.
Using tuples like this would enhance polymorphism, as now a lot of the same
logic can be shared, and in particular, dispatch would always be determined
using a single actual tuple parameter; the automatic generated code that
dispatches a virtual call can simply invoke "isa" once per candidate to
find an implementer.  So then the logic becomes a lot more like the
multi-sub/method dispatch logic of Perl 6, wherein variants of the same
virtual routine can now have different numbers of or names for parameters.
No change to the empty-string-named expr node; it is the function-result.
Inside the routine body, the tuple argument as a whole is like an inner
expr node, in that one can optionally name it (but using : rather than ::=)
and if they don't then a name will be automatically generated, and then for
all practical purposes it would not be explicitly referenceable; regardless
of that naming, all of the tuple's attributes can be referred to using the
special leading-dot syntax, such as ".numerator"; the leading dot syntax is
no longer a shorthand into the "0" or "topic" argument.
Examples:
    function (ResType <-- ArgsType) : ...;
    function (ResType <-- args : ArgsType) : ...;
    function (ResType <-- tuple-type (x : Text, y : Int) where ...) : ...;
    function (ResType <-- x : Text, y : Int) : ...;
Still todo, how to distinguish 2nd from a single named arg in 4th?
Still todo, sugar for auto-declaring auto-nodes from args, eg so we can
just refer to "x" and "y", though the 3rd/4th syntax may just do that
because we declared the tuple type inline.
Tuple selectors need to have special expr node syntax to avoid chicken-egg.
Tuple attribute accessors don't need special syntax, strictly speaking,
because functions don't have to return tuples.
5.  Making procedures also have exactly 1 tuple-typed positional parameter
should theoretically also work, thanks to pseudo-variables, and because
even subject-to-update parameters are always passed values,
a type constraint or virtual dispatcher can include those too.
However, in the general case, we would expect procedures to have *two*
predicates, because there may be values allowed for output that aren't
allowed for input, and vice-versa, in theory.
Related issues are still marking params/args as subject-to-update or not.
Or we could just conceptualize that the single procedure tuple parameter is
*always* subject to update fundamentally, as a function's is read-only, and
any further markings on its attributes, probably only happening if declared
inline, the "&" or absense of said would be more for optimization and
adding of constraints that variables or non-variable exprs are allowed.
Where the 1st and 2nd formats are used, there can also be a separate
declaration list to indicate ro/rw or alias-exprs etc.
I would expect global-params to remain separate from the parameters-tuple.
6.  We should be able to retain existing signature/etc syntax a large part.
7.  See also what Perl 6 does about routine parameters.

* Do away with TREAT as seems to be useless now.  See TTM chat c2011.06.11.

* Recheck my terminology.  Perhaps "consume" is better than "compose" when
referring to what one does with roles/mixins.  See Perl 5.15.3 perlootut.

* Perl 5.16.0 introduces the __SUB__ token, which is a ref to the current
subroutine, so Perl now has a direct counterpart to rtn(), which is nice.

* Bring back, with enhancements, the "transition constraint" support
removed in version 0.140.0 as I changed my mind again on its utility.
As per my comments to the TTM list on 2011 Oct 16, I consider the primary
benefit of transition constraints to protect the explicitly recorded
history of auditing databases from being changed, eg such as with a
"you may only insert" transition constraint.  The transition constraints
can also be used on their own for simpler business rules.
So this restored feature expressly is pure functional / declarative and is
*not* replaced by stimulus-response rules.

* NEXT PRIORITY...
Make the system catalog into something much closer to a *concrete* syntax
tree-like-thing.  See various following TODO items for details.
Mostly do this as its own spec release with minimal dialect/etc/other chgs,
that is, dialect changes to fill in new slots not needed, but anything the
catalog would break could be altered.
Includes:
1.  Get rid of the "scm_" prefix for cat attrs, since we're more formally
storing concrete syntax anyway, so treat those as normally significant.
Also update or remove/move SCM section in Basics.pod since we aren't really
differentiating between "metadata" and normal code anymore.
2.  Add Bool-typed scm_foo cat attr everywhere we have a "name" attr that
says whether the name is user-specified or compiler-generated.  See below.
3.  Refactor the scm_vis_ord and scm_comment attrs as described below.
4.  Refactor the FooSelExprNodeSet types to merge most of them into one
that just stores an ordered list of Name attrs; eg, [rank|vis_ord]:name
pairs; the existing ArySel would be the closest fit.
5.  Add attr for func-invo etc that specifies whether prefix or infix or
postcircumfix etc form was used, where applicable.
So ultimately distinctions of func-invo-alt-syn are recorded in catalog.
6.  Likewise for := op, to say if "x :=foo y" or "x := x foo y" used,
and so on concerning meta-ops, maybe.
7.  Add attr to other expression kinds saying which forms were used,
eg for if-then-else, whether that or ??!! used.  Or for relation literals,
whether ordered or named form used.

* Maybe add Pair and Dict generator types while we're at it; Pair and Dict
have the same binary heading like "(key : Universal, value : Universal)"
where Pair is the tuple type and Dict is the relation type; and "key" is
also defined to be a unary key.
Consider redefining Array to be a subtype of Dict where the key is
a nonnegative integer and its values are dense; in other words, the new
Array is the same as the old one but renaming "index" to "key".
Then you can use a lot of the same operators on Array and Dict like Perl 6.
Theoretically we could also make Bag a subtype of Dict where its "value"
is a positive integer; so then "key" is its payload rather than "value";
this then would also be like Perl 6 and it might be worth it.
But if we do this then we'll have to relook at the set membership/etc ops.
Maybe while we're at it, rename Set's attribute to "key" for consistency.
- Note: The Postgres hstore type may be relevant for a nested one of these,
where I think the keys and values are all text.
- Note: The "hyper" meta-op would be particularly for Dict then, such that
when we say "foo >>op<< bar", then foo/bar are both Dict and the result is
also a Dict, where "key" match the "key" of both inputs and "value" is the
result of applying "op" pairwise to the "value" of foo/bar.  This would
just work for Array and Bag ostensibly.  What to do when
"foo^{key} != bar^{key}" is an open question though; the simplest answer is
that the result just has the set-intersection of the input keys, like a
join would produce; or we might want to define some "outer" version?.

* NEXT PRIORITY...
Reduce the general options for concrete value literals to have just the
simple ones.  For any given "x:y:z", remove all "y" but for Scalar where it
is necessary; people can wrap a literal in an explicit TREAT assertion/etc
otherwise if they really want to.  Also remove all "x" where possible, so
just the plain "z" is the only option, in general.  So then, to write an
integer/rat/text literal, the only option then is to say "42/3.25/'hello'";
you can't say "Int:42/Rat:3.25/Text:'hello'" any more.  So cascade these
simplifications, and we also free up the ":" mostly for other uses.  This
particularly applies to PTMD_STD, but we'll also simply the Perl-STDs where
possible, which is easier in Perl 6.
Unlike "[$|%|@]:..." for generic value literals, only %|@ without the colon
are in use, I believe, as prefix operators, meaning cast-tuple-as-relation
and vice-versa, but there is no colon-less $ prefix in use nor does it make
sense for any similar purpose.
So, use $ for name literals, that is, "$foo" means "Name:foo"
and then say "$$foo" means "NameChain:foo" (doubleup for chain).
And then we're a long way towards being able to ditch the postcircumfix
syntaxes for eg projection since, say "r keep {$foo,$bar}" is terse enough.
Then come up with something for rename, maybe a set of name-pair literals.
After this, keep postcircumfixes rare and common, like for array elements.
In fact, we could just say foo[x] and bar{x} are then array/dict lookups.
Or the dotted forms are elem lookups and no dots are for slices, like Perl.
And then "<expr>.attr" is then its own thing rather than being a shorthand
for "<expr>.{attr}" although it may still be a function shorthand.

* Consider having unary virtual functions for some common kinds of casts,
such as "?" for boolean, "+" for number, "~" for char str.  This terse
syntax would always deal in just base-10 for numbers-with-char.  As for
whether to produce an Integer or Rational or etc from a string, the parsing
rules would be like for PTMD_STD, eg, presence or absense of a radix point
or a division/fraction slash, etc.
Casting to the same type produces the same value as the input.
The functions might be transitive, or whatever the term, so eg saying
"~somebool" produces the same result as "~+somebool" and so on.
Likely booleans would translate to integer 0|1 or string '0','1'.
Likewise in reverse, any number equal to zero, or any string that casts as
a number equal to zero, would become bool false, and otherwise bool true,
at least speaking for strings that successfully numify.
So 0 and 0.0 and '0' and '0.0' are false, and other numifyable are true.
Attempts to numify a string that isn't a number will throw an exception.
Attempts to boolify any string will succeed, where the above zeros plus
the empty string are false, and other strings are true; to be specific, any
string that would successfully numify would use numeric rules for truth,
and any string that doesn't successfully numify would be true unless empty.
ON THE OTHER HAND, it would probably be much better and more predictable
and extensible to forget about transitivity, and so, boolifying a string
will only produce false for the empty string and true for all other strings
and to get the '0'=false behavior you have to say ?+str; MUCH BETTER.
Unlike Perl 6, the prefix !foo will *not* be system-overloaded to mean
!?foo as doing so would likely introd accidental logic errors of implicit
casting when one considers ! is widely used when just bool inputs expected.
Similarly, unlike Perl 6, no other ops will implicitly cast like infix ~,+.
If this is adopted, "so" will therefore be virt rather than a "not" mirror,
though for the boolean composer it becomes the same as otherwise.
ON THE OTHER HAND, since there is so little standardization and wildly
different expectations on what strings are considered true or false, it
would seem best for strings to *not* provide system-defined terse bool
casting at all, and instead force people to be explicit, which would
typically mean either going by way of a number, either from a numify cast
of the string or a length-test of the string, or some explicit boolean test
like comparing the string against a list of values.  Similarly, a boolify
of many other things like collection values is best done by way of some
intermediary like a number such as a length or elements count test.
So eg the so/not-empty prefix for a relation/array is then ?#r or !#r for
consistency with "? #r" or "! ? #r", and #?r or #!r is probably invalid.

* Grammar spec related to separating/trailing chars like ',' or ';' could
look like this:
  <foolist> ::=
    <open>
      [<item>? <sep>]* <item>?
    <close>
... which says every item may have a trailing separator, and every item
must have a trailing separator except the last/only one, and any item may
have a leading separator or separators may appear several in a row; the
semantics are that extra separators do not introd any implicit extra items.
The above would be for a 0..n list; here is a 1..n list:
  <foolist> ::=
    <open>
      [[<item>? <sep>]+ | <item>]
    <close>
... but there are probably cleaner versions of both.

* The SQL-92 Standard CREATE ASSERTION is like a generalized db-level CHECK
constraint but few SQL DBMSs support it.  Note this in the spec.  RDB from
DEC had it, but they got bought by Oracle when DEC went out of business.

* FYI, PgOpenCL exists, run intensive db tasks in the GPU.

* ALSO...
Get rid of the Set.new() options in Perl 6 and generally update the
Perl-STDs to use just Array/Seq/arrayref and mostly not set/bag/hash/etc,
partly for code brevity but particularly to preserve the visual order of
elements from source to catalog and back again.  Also add scm_vis_ord to
the catalog or change some catalog types to record order.

* Note that Perl 5.15.4+ fleshes out / completes the support for having
Unicode in identifiers, to treat them properly, such as not ignore the
UTF-8 flag; it also fleshes out support for the null character "\0" so that
eg regexes with them work properly.  See perl5154delta.

* Also add to Perl5-STD explicit support for some more Perl modules whose
objects we would implicitly treat as built-in scalar values.  We already
have BigInt, BigRat etc, but add Ingy's "boolean" (which is expressly
intended for cross-language interchange in its POD) and Juerd's "BLOB".

* Maybe also but probably not yet be concerned w comments in Perl-STD/etc.

* NEXT PRIORITY...
Reformat all declarations, materials/subdepots particularly, to be of the
format "name ::= kind ..." rather than "kind name ...".  Similarly, we may
be able to just nix the "subdepot" keyword so a subdepot is then declared
as just "foo ::= {...}", and a material as "foo ::= kind ...".
So the name on the left of the ::= is no longer part of the material node
itself but rather is part of the larger thing into which the material node
is composed; the material node is now just eg ['function', <payload>].
Material nodes themselves just declare anonymous entities.

* Rename the "subdepot" concept to "schema", even if this term is broad
enough to include what Oracle calls "package" and not just SQL "schema".

* Example of function in Perl with context ...
The FunctionSet tuple:
    ['cube','opt comment of FunctionSet',<Function>]
The Function tuple:
    ['opt comment of Function',<heading>,<body>]
... and so on.  The comments are first so they're like leading comments
as is common with whole-routine definitions and such, and they tend to be
terser besides.

* ALSO...
Change function bodies from {...} to (...).  And make "..." expr/stmt kind.

* Likewise make a code comment a stmt/expr kind or otherwise provide for
specifying where it goes visually in code in a statement position.
Consider reworking scm_vis_ord to be external for some things it describes,
eg mapping an order to a declared name.  In fact, if this is done, then it
becomes much easier to add/remove/reorder code pieces because their
sequence numbers are stored separately and so code diffs on the system
catalog itself may not show much in the way of spurrious diffs, especially
if the mapping is simply an array_of.Name.
Consider pulling out code comments in a similar fashion, just putting them
as their own named (as stmts/etc are named) code bits, which are then
associated by name with other code bits, and are listed in the vis-ord too.
In fact, then where comments are physically visible and what things they
are semantically connected to are then not joined at the hip.
The comment names are optionally user-specifiable too, like with statemnts.
For example:
    cmt_on_x ::= C:'This roxors!';
... or:
    comment cmt_on_x ::= C:'This roxors!';
... and other details still to fill in like saying what it applies to.
Maybe a new infix-op-bind-like syntax will fit the bill for association.
Also make blank code lines or visual dividing lines recordable in syscat.
See also Postgres "COMMENT ON ..." syntax.

* Note: Postgres $$...$$ delimiters are actually general string delimiters
and can be used anywhere '...' are, not just function definitions.  They
also generalize to heredocs, eg, $foo$...$foo$ so maybe PT_STD should
support this kind of thing too, maybe.  For that matter, I should have a
function that takes a string as input and outputs some delimiter suitable
for quoting it without escapes because it doesn't occur in the string.
This, as well as other escaping functions, would be used by a code gen.

* Don't use any quote-like characters for delimiting code comments after
all, but instead use nonidentical multi-symbolic-character delimiting
tokens, such as a "/* */" pair as in C/etc.  The tokens would be ones not
valid as normal operators, and they would be visually "heavy" rather than
"light" such as quote-like string delimiters are.  To make it easier to
comment-out code blocks that may already contain comments or have literal
strings that contain comment delimiters, the comment delimiters may be
extended to an arbitrary length using repetition, where both ends must
match.  Perl 6 is used as inspiration for this.  We would presumably use a
bracketing character for the extender, for example "/*{ }*/" or "/*{{ }}*/"
etc.  This means there is NO ESCAPING in code comments for the comments.
On the other hand, maybe some certain escaping is still needed such as for
whitespace like linebreaks and indentation, as with text/etc literals, to
control exactly what is captured, or we have a version of comment for each.
Apparently both Tutorial D and SQL support /* */ block comments too.
All Muldis D comments are block comments on purpose; there are no line
comments, because of a desire to make it line-ending-chars-generic.

* Invent some lightweight syntax for the general case of infix operators,
whose use is optional for system-defined infixes, so users can define infix
operators or invoke their routines infix, and the parser will interpret the
infix-using code correctly in a context-free environment where the parser
doesn't have access to other user-defined-stuff definitions, mainly telling
routines apart from variables, since the latter don't have special markings
on purpose.  This syntax will need to be something that users or Muldis D
are unlikely to want to use for some other purpose.  By "lightweight" we
mean something that is visually small but also short to type.
One possibly best choice is the GRAVE ACCENT; for example:
    x `foo` y `bar` z
... or:
    [`foo`] ...
... for user-def reduction, which couldn't be "[foo()]" for arry confusion.
But then we'd need something else for code comments, though code
comments are probably much more amenable to having double-delimiters;
or better yet, lets use single-quotes for comments now, same as for Text or
Blob, and have some kind of prefix like with a Blob to know its a Comment.
We could similarly use that for user-defined prefix operators sans parens,
where context (terms-in-a-row/etc) could differentiate from infix, although
the value of this is lower relative to foo() compared with supp infix.
We would probably never support unary postfix or circumfix operators,
save the few built-in special syntaxes.
Lets just say that there is no part of the catalog for declaring routines
as infix but that rather any user-defined (or system-defined routine can be
used both "foo(x,y) and "x `foo` y" iff it has exactly 2 positional
parameters that are mandatory (names are "0" and "1") and so any other,
optional, parameters may only be caller-specified in the "foo(x,y)" form.
The system catalog would specify if that a routine *invocation* is foo() or
infix etc but the routine *definition* knows no difference; both forms use
exactly the same choices of routine names, whether alpha or symbolic.

* A tangent to the above is that where appropriate we can take any function
of 3+ parameters which conceivably would be made easier to use in a
ternary+ infix format and make a set of dyadic functions that break it
down, kind of like how the creation of a distinct "assuming" function
allowed numerous 3-arg functions to generalize into a pair of 2-arg.

* Note, apparently Haskell has no prefix operators except for unary minus,
and normally all of its infix operators are symbolic where normally all
alpha operators must be used in "foo()" notation.  Also, you can take a
foo() operator and use it infix using backquotes, eg "x `foo` y", meaning
my thought of doing this has a precedent.  Haskell says you can take a
symbolic infix and make it prefix using parens, eg, "x + y" -> "(+) x y".

* PTMD_STD Syntax/parsing cluster:
- A general rule is, the parser has zero knowledge of what specific
operators or vars/etc exist, and must be able to derive a concrete syntax
tree using just basic grammatical knowledge, such as knowing the difference
between whitespace and alphanum and symbolic and quoting and bracketing
characters, and so the parsing rules are based on those restrictions.
In particular, the behavior of a parser or how a piece of code is parsed
does not vary depending on what user-defined entities are declared at the
time, which should aid on security and predictability and simplicity.
There are only a small amount of disambiguating rules that say certain
tokens are system-defined operators, so otherwise no knowledge.
The hardest work then, in the face of Unicode, is specifically identifying
what characters count as alpha or symbolic; eg what are Greek letters?
Generally speaking we would whitelist; any characters or groups we don't
whitelist into a particular category are disallowed from appearing outside
of a quoted context.
- Remove dash from bareword ident, leaving just alphanum and underscore;
people can quote names with dashes when they want them.
- There are no postfix operators; eg, ++ is prefix.
- Namespace-qualified/dotty operator calls must be in foo() syntax; plain
prefix or infix operator calls must be unqualified.
- Whitespace affects parsing.  Any consecutive 2 alphanum or 2 symbolic
tokens must have intervening whitespace.  Quoting '"` or bracketing (){}[]
characters count as neither alphanum nor symbolic in general.  Whitespace
may be omitted between any 2 of these distinct things in general: alphanum
tokens, symbolic tokens, quoting or bracketing characters.
- Default to treating any bareword run of 1+ alphanums as a var name or
foo(), treat any bareword run of 1+ symbolics as a non-foo() operator.
- All bareword symbolic tokens are plain infix/prefix op calls.
- All grave-accent-delimited (`) tokens are plain infix/prefix op calls.
- All double-quoted (") tokens are var/etc references or foo() calls.
- All single-quoted (') tokens are string value literals.
- All namespace-qualified/dotty tokens are var/etc refs for foo().
- All tokens followed by an open-paren with no ws between are foo() calls
if the token is bareword alphanum and a plain prefix op if symbolic.
- Any code enclosed by a pair of bracketing chars is treated as a var/etc,
except when they contain only a symbolic token and no whitespace, in which
case they instead are taken as an extension of said symbolic token, eg [+].
- When 2 or more plain op calls appear consecutively, all but the first are
taken as prefix ops; the first op is taken as an infix op if it follows a
var/etc and a prefix op if it doesn't.  Eg "x + -y".
- When a symbolic token appears between 2 alphanum tokens, the symbolic
token is taken as a plain prefix op if there is no whitespace between it
and the following alphanum and there is whitespace between it and the
leading alphanum, while it is treated as plain infix if there either is or
is not whitespace on both sides, or there is whitespace just following it.
- When disambiguation isn't present, sequences of bareword alphanums are
interpreted as alternations of var/etc and plain infix op calls, always
ending with the former, like "var op var op var" or "op var op var".
- Certain bareword tokens are special-cased to always be interpreted as
prefix ops, namely {not,so} (there might be more).
- All operators with more than 2 parameters must be called in foo() syntax.
- All prefix ops have 1 param named "0", all infix just 2, named {"0","1"}.
- All bareword prefix ops are symbolic except for a small number of special
cases such as "not" and "so"; all other bareword alphanum ops are infix.
- The only infix syntax taking more than 2 inputs is special syntax with
its own kinds of parse nodes, not op calls, such as ??!! and if-then-else.

* We have just a small number of precedence levels, here from highest to
lowest, loosely speaking:
  - base literals or selectors or delimited whatevers or foo() etc
  - general symbolic prefix (tilde-quotes-or-bareword make no difference)
  - general symbolic infix (tilde no diff)
  - general alpha prefix (tilde no diff)
  - general alpha infix (tilde no diff)
  - general other prefix (tilde-quoted only option, can't be bareword)
  - general other infix ("other" if any chars outside symbol+alpha set)
  - short-circuiting / ??!! / if-then-else / given-when-def
  - assignment := (base statement-only)
  - binding infix ::= (binds to statement if possible, else expression)

* Pegex grammar should work at some level midway between a pure tokenizer
and the resulting AST.  It should form trees where easy to do without
non-trivial lookahead.  In particular, sequences of barewords and etc where
interpretation of what is a pre/infix op and what is a var/expr/etc depends
on the number of barewords and what they're next to, should be output as a
flat list and no attempt to form a tree in the grammar.  Also, the Pegex
grammar perhaps should capture everything, including ws, just in case.

* IDEA: Add a concept kind of like a distinct depot/package catalog but it
is system-generated and not directly editable by the user; in here would
live copies of all of the canonical type definitions, especially scalar
types, from both the system and user namespaces.  The copies in this
package are the ones cited by a ScalarWP value in the low-level type system
as being the scalar type name.  When there is just one normal copy of the
type def, it is more of a redirection.  When multiple user depots declare a
copy of a type and then the mounting commands further specify that they are
to be considered the same data types, then just a single copy exists in the
new sys-gen package for both, rather than one for each.  This has various
benefits like saving a chicken and egg problem concerning say transient
values in the system when various depots that may declare types for it can
come and go.  It means the low level type system type name for an
(immutable) ScalarWP value never has to be changed for the life of the DBMS
process no matter what happens with the depots user types are declared in
or as copies come and go.  For that matter, we can just identify this copy
of the type name with a plain integer, this separately being mapped to
namechains for the various normal copies it is aliased with, so we get
memory efficiency too.  This all probably has wider design implications or
can inspire other changes.

* Consider the provision of a limited search path functionality for
user-defined functions and types so that they could be invoked tersely or
unqualified like builtins, particularly so reasonable infix syntax or
singleton types could be supported.  The fully-qualified invocation of such
a floating name would have another special name token, in the same way that
"par" and "nlx" etc are special.  Maybe "shp" (SearcH Path).  So, in the
system catalog, all unqualified names "foo" become explicit shp-prefixed
names "shp.foo", which includes references to all system-defined routines
as typically used.  On a tangent, we can revise how NameChain are expanded
in the system catalog, due to wanting to make it as close to the user's raw
syntax as possible, so eg not expanding unqualified to sys.*; if necessary
for ease of scanning the catalog, we might store 2 versions of a namechain
together, where one is expanded and the other isn't.  A "shp.foo" would be
interpreted as follows: 1. first all system-defined possibilities would be
explored and exhausted; 2. then user-defined possibilities that are
siblings of the invoker or siblings of ancestors from closest to farthest,
and then failure.  To be specific, in a given "shp.foo.bar.baz", just the
"foo" is looked for as siblings of ancestors, and then the first one that
is found would then be dug into as usual, to succeed or fail, same as if
one had said "par" instead.
A "shp" can be included along with other special chain
elements so to further customize the search path so that say #2 can be
followed relative to a specific place in the namespace rather than from
where it is actually invoked; for example "fed.lib.db.foo.bar.shp.baz" but
that this example wouldn't actually work, so details still to figure out;
on the other hand, we could say that this last feature would be unnecessary
because the existing APMaterialNCSelExprNode feature would make stuff work.
Or possibly APMaterialNCSelExprNode/etc could be employed with an expanded
role to translate shp.foo to abs-paths which then could be used as usual.
And so, the parser doesn't have to resolve paths so much from unqualified
to qualified, this functionality being pushed back, but instead the parser
still has to resolve what a reference is to, whether a function or a type
or a data-entity etc so users will still have to provide enough clarity in
their syntax to disambiguate this.

* Note that Postgres and MySQL both support an UPDATE statement extension
where you can mention multiple tables besides those being updated, where
semantics are that all of the tables are joined with the one being updated
and so you can use data from other tables in the SET clause.
This is a *very* useful feature and Muldis D should have an analogy,
or this feature would be helpful in implementing Muldis D features
like certain kinds of relational assignment involving self joined with foo.
As of Postgres 9.1 the UPDATE also supports the WITH clause.

* Note, http://facility9.com/2011/12/ten-reasons-postgresql-is-better-than-sql-server/
which gives some interesting details or explanations of Postgres (per 9.1)
features, including use cases for writeable CTEs, and compares with SQL
Server; a response also lists some of Postgres' weaknesses in comparison.
Also, re unlogged tables, and serializable transactions.

* Note, see http://en.wikipedia.org/wiki/Vector_clock and such things.

* Note that in Postgres, db encoding SQL_ASCII just means text has no
encoding and Postgres just treats it as bytes, so any values can be stored.
Other encoding choices are basically constraints to ensure your
data is valid according to that encoding.

* On 2011 Oct 17 I made a comment on Andrew Dunstan's blog that in the
context of foreign keys, making changes to parent and child
tables simultaneously in a single statement is much better than trying to
determine correct order of operations.  In response Andrew said "That might
be possible in 9.1 with writeable CTEs, but I happen to be on 9.0 with this
client, and the constraints are not deferrable."

* Note that Postgres databases default to SQL_ASCII (7-bit) encoding when
an explicit encoding (such as UTF8) is not specified upon their creation.
"The SQL_ASCII setting behaves considerably differently from the other
settings. When the server character set is SQL_ASCII, the server interprets
byte values 0-127 according to the ASCII standard, while byte values
128-255 are taken as uninterpreted characters. No encoding conversion will
be done when the setting is SQL_ASCII. Thus, this setting is not so much a
declaration that a specific encoding is in use, as a declaration of
ignorance about the encoding. In most cases, if you are working with any
non-ASCII data, it is unwise to use the SQL_ASCII setting because
PostgreSQL will be unable to help you by converting or validating non-ASCII
characters."

* ALSO...
Use colons to separate any kind of heading/body pairs, both materials and
values.  Take Relation now "@:[...]:{...}" as example to follow.
Also, routines now "function (...): (...)" or "updater (...): {...}" or
"procedure (...): [...]"; this for routines is inspired by Python.  This
then opens the door for routine body bounding chars to be opt sometimes,
and makes clearer where a heading ends and a body starts when there are
various extra heading clauses such as is-x or implements x.  Also consider
using ":" in other places where pairs are, maybe freeing up => for
something more specific; eg Python uses ":" in dicts rather than =>; or do
the opposite; keep "=>" for named param/attr/etc lists and use the ":" for
things like Bag literals or generic dicts that are binary relations ... use
one for atvl:atvl (bags/dicts), other for atnm:atvl (tuples, arg-lists).
Also consider using "::" for something, maybe type conversion, as Pg does.
Keep "::=" as for explicitly associating names with what they are naming.
DESPITE WHAT MNEUMONICS SAYS, lets use the : for name/name and value/value
pairs within delimiters also (unless <- still better for rename) and so
maybe the only place => is used is as a binary infix op to construct Pair
tuples, same as .. is an infix op to construct intervals maybe I guess.

* Have ";" as separator (opt lead or term) for both statements/vars/exprs
etc as well as whole materials.  This also comes together nicely for the
simpler routines that don't need to have bounders because they are just
single statements or expressions, for example:
    cube ::= function (Int <-- topic : Int) :
        topic ^ 3;
... and that's it.

* Remove the "var" and "attr" keywords or make them optional noisewords.
Simply having "foo : bar" in a procedure statement position should be
enough to know it is a variable declaration.
Likewise, "foo : bar" in a sca/tup/rel typedef can be known an attr def.
Then, other things can gain optional noisewords, such as "result" before
the type in function sigs, or "param" before a param in routine sigs,
or "expr" or "stmt" optionally before those things in a routine, etc.

* Update system catalog, if necessary, to support specifying where a named
expression, or a variable declaration, lives visually in a statement list.

* Consider dropping the special support for dot-accessors as their own
expression node kinds in the system catalog, meaning AccExprNodeSet, and
code can just use an ordinary function invocation on Tuple.attr() instead;
or that node kind can be downgraded to just be doing aliasing, like when
you write "a ::= b ::= c" (target is then just a Name, not a NameChain).
This change would improve internal language consistency.
As part of this change, the formal syntax "t.{x}" goes away so just "t.x"
remains.  Then "." becomes an ordinary dyadic infix operator in the context
of referring to a data entity, same as "."(t,x) where "." synonyms "attr".
But when referring to a routine/type the whole dotted name is a NameChain.
But if "." is just an infix operator, then "t.x" means
"attr(<value-of-t>,<value-of-x>)" so things aren't actually that simple.
Regardless, if there may be separate "." for relations or scalars then
"t.x" may actually point to a virtual op, only func forms disambiguate.

* NEXT PRIORITY...
Add support for material and parameter synonyms.  And change what params
any positional arguments implicitly go with from topic|other to 0|1|...
But don't actually change any routines/params until later, except adding
0|1 to all topic|other.

* Update the array-specific postcircumfix concrete syntaxes to make them
more generic such that the array index/es (what's inside the "[]") may be
any arbitrary value expression rather than having to be an integer or
interval literal.  But if nothing else changes, this means the slice will
have to be spelled like "ary[{x..y}]" rather than "ary[x..y]", but
individual element access like "ary.[x]" will still work.  But now you can
actually have the x,y variables rather than those having to be constants.

* Consider taking a more Perl 6 like approach by turning ".." and its 3
friends into infix dyadic functions that take endpoint values and result in
interval values.  Then the surrounding curly braces are no longer needed,
and you can once again say "ary[x..y]".  Note that if ppl still want/need
delimiters for an interval, they can always use parens, like "(x..y)".
If we also redefine an MPInterval to be a set_of.SPInterval, then any
{x..y} would unambiguously mean either a set or MPInterval, but we may then
lose the shorthand "x" meaning "x..x", but this could be ok tradeoff.

* Consider also making the likes of "," and "=>" into dyadic functions
along the lines of Perl 6, though this would have further consequences.

* Add boolean monadic function "so"/"?" which returns its argument;
it serves as a useful noiseword in code, helping parity with "not"/"!".

* Have a Boolish mixin-union type which Boolean or Bit etc compose.
By having "so" and "not" in this union type, users can compose Boolish
into ostensibly less-Boolean-like types like numbers or strings and so they
can define implementations for "so" and "not" that are like the Perl
operators when one treats numbers or strings like booleans.
But the system-defined numeric and string types *won't* compose Boolish/etc
as we prefer stronger typing or more explicitness by default.
The details of this will require more thought, maybe more mixin types.

* Account for that we can't generally have a virtual N-adic operator that
accepts the empty set as input, where that is implemented by type-specific
N-adic operators that accept the empty set as input, because it wouldn't
know which type-specific N-adic to dispatch to for the empty set.
Instead, make the type-specific N-adic virtual with 2 implementers, one
taking just the empty set and one taking nonempty sets; the type-specific
0..N virtual will not implement the type-generic 0..N; rather the
type-specific 1..N will implement both 0..N virtuals directly; the
type-specific 0 will only implement the type-specific 0..N; there would
also be, as applicable, a type-generic 0 implementing just the generic
0..N.  Eg, so we have just Num-0..N, Num-0, Int-0..N, Int-0, Int-1..N at
least for ops that have identity values, and no -0 where there aren't.

* Consider creating an analogy to virtual routines that is to existing
virtual routines what domain-union types are to mixin-union types.  That
is, define a kind of virtual that declares what other routines implement
it, and so is not user-extensible.  This is essentially an alternative way
for users to write wrappers for related routines that dispatch on argument
types.  An example is we can have separate "ungroup" and "unwrap" functions
that take only nonempty relations vs those that take empty ones too; the
nonempty-only ones don't need the extra parameter to say what attributes to
extend with when the relation is empty.  Or maybe those should be normal
functions and better examples for the new kind of virtual will come around.

* Don't worry about declaring identity values somehow attached to dyadic
function definitions in the catalog; instead, use the virtuals mechanism
we have to just declare the triple {0..N,0,1..N}, um, or something.

* Make all system-defined functions generally return special values on
failure rather than throw exceptions.  For example, make division return
the NaN.DivByZero singleton and so on.  But functions whose whole role is
to assert, such as treat(), would still throw actual exceptions, and some
other kinds of problems may be better suited to thrown exceptions.  This
gives users the choice to either explicitly accept such situations or not.
If users don't handle such situations, then often-times they will
immediately get a type constraint violation exception (which is an actual
exception, not a special value), and the description of the exception
message is still informative enough, eg
"Nan.DivByZero isn't a value of type Integer".  This works because typical
system-defined type-specific functions will not accept special values as
input, even when they might return them, so in nested expressions we still
get exceptions in about the same places for the same reasons.
But now the Muldis D analogy of "@foo = map { $_.x / $.y } @ints" in Perl
won't itself throw an exception, if @foo is a Universal-array, but it will
throw an exception if @foo is an just-Integer-array.
One could say that a significant portion of the exception throw/catch
system has been made redundant by the type system.  For example, a routine
signature formally declares exception-like conditions as its result types.
I suppose one might say this could lead to action-at-a-distance problems
such as what exceptions are meant to help prevent in the first place,
but when code is written with fairly narrow declared types, then errors
would not tend to get very far before detection, I would think.
Now subject-to-update parameters of procedures can be tricker, because what
if the param decl type is Text|IOErr but the variable argument is a
just-Text var?  Is that something we would expect to fail at compile time
or just optionally at runtime only if a IOErr is to be returned?
Make sure *don't* call the special values "exceptions"; use something else?
See also the list of IEEE float special values, and Mathematica/etc such as
http://mathworld.wolfram.com/Indeterminate.html / etc.

* Support at least a base level of controlled override-overloading with
virtual operators, meaning where multiple impls overlap in their domains.
Normally, if several implementation signatures match the arguments to the
virtual, it is undefined which one is called, and in general it is onerous
to determine which one is more "specific" than another to pick that.
The base proposal is that the virtual operator itself can name, or be its
own, default implementor, where this default is invoked if all of the other
implementors don't match the arguments, rather than there being a type
constraint violation.  Actually, best for a virtual to not be its own
default, so users are able to always invoke the default implementation
directly without worry of it being overridden by something.
A natural extension to this is that in any cases where an implementer of a
virtual is itself a virtual, it can do likewise, providing a default.
In this way, on an operator-by-operator basis, we can support a hierarchy
of sorts like in a multiple-inheritence OO system.
This idea still needs thought to flesh out details of course.

* Demote the numeric operators that are more statistics-oriented from the
language core into a new Statistics extension or some such.  Specifically
this means these 5 in [Numeric|Rational|Integer]: range, frac_mean, median,
frac_mean_of_median, mode; and these 2 in Integer: whole_mean,
whole_mean_of_median.  Also, this "mean" is "arithmetic mean" (division of
sum); there is also "geometric mean" (root of product), etc.  After the
demotion, this set of ops can be changed or expanded to be something more
appropriate for statistical applications; some yet-missing SQL-standard
functions like pop-etc can then come in also.  Now these core-removed
functions are just shorthands for not-too-complicated expressions that
users can define for themselves with core ops, so they're not really
missing anything important if they only get the core.
For example, the current (arithmetic) mean is just:
    arith_mean ::= function (Rat <-- topic : bag_of.Rat)
        ([+]topic / #+topic)
... and geometric mean is something like:
    geom_mean ::= function (PRat <-- topic : bag_of.PRat)
        ([*]topic ** (0 - #+topic))
... but any vers in the dedic Statistics could be impl more efficiently.

* Note that general case of "median" is "quantile" (median is 2-quantile).

* Drop special entity name embedded support for inline type declarations
like "foobag : bag_of.Foo"/etc; instead, this syntax is demoted to a
dialect-specific thing that is just sugar for something like "foobag :
relation-type Bar { attr value : Foo, attr count : PInt, primary-key {
value } }".  That way, we can always point to a specific material that
actually exists when asked what is the declared type of "foobag", and also
we are psychologically more free to just declare things as relation types
anyway, and the added flexibility that comes with that, such as in the
definition of the system catalog itself, and also then the concept of an
entity name chain is no longer overloaded.

* Generalize the Set/Array/Bag/Maybe-specific operators so that: 1. the
names of the value/index/count attributes can be specified with arguments
(that are optional, and default to the current ones if not given); 2. they
work with relations of arbitrary degree.  For example, merge the Counted
extension into Bag and call it Counted, and generalize Array into Ranked
("Ordered" is already taken and best left as is) which also absorbs the
ranking and quota functions from Relation.pod, and generalize Set into
Relation.  The Counted|Bag is then any 1+ degree relation with a
positive-integer typed attribute C that has a key (or superkey) on all of
the attributes except for C; it is treated as special by the functions,
which are analogies to general relational functions that work as normal on
all attributes but C and merge C.  The Array|Ranked is then any 1+ degree
relation with a nonnegative-integer typed attribute I that has a key on I
and is further constrained that "max(r{I})+1 = #r"; I is treated as special
by the functions.  The Maybe is then any relation with a nullary key.  With
these generalizations, some concrete syntax like .[N] will just compile
into special cases such as assuming certain special attribute names, and
you can use the foo() syntax when that isn't the case.  After these
generalizations, some Counted|Array|Maybe|etc functions can be core and
others can be pushed into extensions, as is appropriate.  After these
generalizations, we may or may not still have named Array|Bag|etc types,
which will probably keep their definitions, as special cases of the
generalized where the attribute names match the canonical ones.  Also
rename "index" to "rank" in Array perhaps.  After the
generalizations, the distinct usefulness of Set would decrease somewhat.
Note: For a generalization of Maybe, consider the Zoo name, inspired by
Database Explorations that discusses MD's canonical missing info solution,
or alternately call it C01 in the spirit of D0C0/D0C1/D0.
Still in question is what if anything to change about [S|M]PInterval/etc.

* Consider removing MPInterval as a s-d type and rename SP to "Interval";
then, either one can just use "Interval" as a "Set" element to get the same
effect, or a relation over "Interval" can at least be demoted from core.

* Add official support for functions/expressions to be able to do some
things that they otherwise couldn't, such as have side-effects or be
quasi-non-deterministic.  To be specific, add support for side-effects that
occur external to the current in-DBMS process, such as output via some
side-channel like STDERR or a message queue, which can be used for
debugging a function.  But any such functionality can't directly affect the
current process, and in particular it can't affect the
function/expression's result value.  On the other hand, it is acceptable
for something to cause the function/expression to abort with a thrown
exception, since this isn't changing the result value.  There should be
metadata for any function which does or might do something like this, to
declare the fact.  In addition, we could support a limited form of
non-determinism, such as allowing a rand() or now() function that does
affect the calling function/expression's result, but that this is
constrained to be mutually deterministic within the whole of a single
Muldis D multi-update-statement.  That is, given the same arguments (or
none), now() would always return the same value within a
multi-update-statement, and might only change between different
multi-update-statements, and rand() likewise.  This might also give some
support for partial-sort functions, as long as they are consistent within
multiple calls in the same multi-update-statement.  Once again, such things
would need to be tagged with metadata.  Normal deterministic functions
always have the same result no matter how far apart.

* Make autonomous transactions / in-DBMS processes not so much startable
directly by a process but rather that the kernal/etc process always does it
directly and any other process asks to have such done by sending a message
to the kernal.  Similarly, DBMS-clients just become message passers, and
they start a process the same way as internally, by sending a message to
the kernal/etc to please call this procedure for me, and the result to the
client is also a message.  This also generalizes the stateful/stateless
thing and streaming/cursor or not thing.  Now also tied into this is
stimulus-response-rules, in that all stimuli are messages.  The kernal can
also initiate messages, such as this depot did mount, or whatever.

* Note that Oracle's autonomous transaction support looks like this:
    create procedure foo as pragma autonomous_transaction; begin ... end;

* Quoth http://ledgersmbdev.blogspot.ca/2012/09/or-modelling-interlude-postgresql-vs.html :
"A simple description of the difference [between MySQL and Postgres] is:
MySQL is what you get when application developers build an RDBMS.
PostgreSQL is what you get when database developers build an application
development platform."

* For real work projects in the short term where one would conceivably use
a Postgres enum, I should just use a "text check value in ('foo',...)"
instead, as it is much easier to manage/change those types.  For example,
if we want to temporarily add extra enum values outside normal range for
testing so the system ignores those values, eg status_code='WASQ'.

* Consider relaxing the restriction of how much of a depot must be defined
just in terms of itself.  So, for example, only a depot's data types (and
dbvar) must be defined wholly internally to the depot.  But any routines in
a depot may invoke routines outside of the depot if the former aren't used
in the definition of a data type or dbvar.

* Consider adding some way of generating a type specification from a value
of that type and consider having something like a system catalog which
describes the actual database value rather than a prescribed database type,
such as to help introspection of a database whose declared type is just
'Database'.  The MST thing of TTM may tie into this.
See also how the "Pick" DBMS works, or something.

* Add a scm_foo to the system catalog next to any place that declares a
DBMS entity name, particularly an expr/var/material, to indicate whether
the declared name is considered explicitly user-specified or parser-gen.
There may be more than 2 possible values (making this an enum rather than a
Bool) that relate, say, to distinguishing explicitly named but inlined
items versus explicitly named and not inlined items.  The sys-cat might
restrict based on this such that it doesn't allow certain references to
entities whose names are marked parser-generated, because any generated
source code would have to make the references visible.  A related
implication is that any entity names marked as generated are not sacred and
are free to be automatically renamed by different catalog-updating actions
such as source code optimizers.  Maybe also have something to distinguish
things declared in positional format so "0"=> etc don't appear, maybe.

* Numeric updates ...
See http://archive.adaic.com/standards/83lrm/html/lrm-02-04.html .
Excise the M;N format for bases 17..36 leaving just 2..16, absolutely.
Use "#" as separator rather than ";".
Write M as a base-10 integer rather than a single character.
These are more like Ada "based" literals then, read better, frees up ";".
So 16#FF is an integer, 16#'FF' is a blob.
Maybe also add Perl-6 inspired commalists, like this:
60#[43,5,12] (integer); no good reason for a blob analogy.

* Define some generic framework for units and measures, so to make it
easier to perform the large fraction of math involving such things, and it
can be more strongly typed and bug free.
The core of the system would have the domain-union type "Measurement" (or
more general), whose composing types should be named after the kind of
thing being measured (eg, "Mass" or "Duration") or the names of the
relevant units (eg, "Kilograms" or "Seconds"), and there would likely be a
variety of subtypes that are union types themselves for further
categorization.  These likely all scalar types.  Subcategories include:
1. Single-unit (eg, just seconds) or multi-unit (eg, any/all of YMDHIS).
2. Approximate (carries amount plus margin/sigfigs) vs exact (no margin).
3. Semantics, such as where versus howmuch, or continuous v discrete field.
4. What can be added or differenced or multiplied or divided etc, either
two of the same measure or a measure and a bare num, what result type is;
we may possibly have a separate domain-union type for each of those.
- When units are always directly convertable, they can be possreps of the
same scalar type, such as {seconds, minutes, hours}?.
- When not directly convertable, should be separate types, such as either
seconds vs months or ...
- For simplicity, we could consider all units flat, such that rather than
adding multiple dimensions orthogonal to everything else, we would just
have MetresPerSecondSquared etc types, this especially because many
plain-sounding units are actually defined as such, eg "Watts"="Amps*Volts".
- With this very basic structure, nearly all the complexity would be in the
operators, and generally one would explicitly define separate functions
for every kind of unit-measures they wish to pair up; eg, dividing distance
by time to get speed would be "(MetresPSecondSq <-- 0:Metres, 1:Seconds)"
but of course that func could overload the generic "/" virtual, +just work.

* To simplify the units and measures framework to not have to deal with
margin/sigfigs directly, we could have a framework for general inexact math
that tracks margin/sigfigs, but this is probably no more complicated than
supporting rationals when we have integers.

* IN PROGRESS ...
Rewrite/update anything talking about matters affected by process isolation
to both declare that Muldis D is generally orthogonal or agnostic to such
matters and makes no guarantees in general that any routine, even a recipe
or updater/function invoked by one, will see a consistent view of the
database during its execution, and generally remove "atomic" terminology,
and rename "nested transaction" to some other terminology.  Rather, any
guarantees of serializability of a recipe/etc will need further work by
users such as to explicitly configure their isolation or locks or whatever
as appropriate, and of course everything's affected by what DBMS you use
and what concurrency models it supports, such as locking or MVCC.  Likewise
the model being used affects when conflict errors may manifest, eg at
commit or earlier, or when/if user tasks will block, or how complicated it
is to resolve or avoid a conflict.  Matters of the concurrency model or
isolation are best not legislated by Muldis D but be left up to the
implementations and users.  Muldis D just has to require that the database
is always in a consistent state on statement boundaries et al.

* Define how one can split a PTMD_STD depot into multiple text files since
you would conceptually put an entire potentially large program in one.

* Tweak the STD dialects to account for defining system modules with them.

* Update STDIO.pod and Cast.pod concerning the Text types split.

* PACKAGE:
- Support variant of "<[ a..z A..Z _ ]><[ a..z A..Z 0..9 _ - ]>*" nonquoted
name strs that's more liberal "<[ a..z A..Z 0..9 _ - ]>+" for just atnms,
possrep names, param and arg names, so any of "-foo", "3", "-4" can be bw.
- Update system catalog and grammars to add lightweight aliasing support
for whole materials, as a new "synonym" (name?) material.  These have no
mutual order but the actual non-synonym target is the "primary" name.
Grammar can be "synonym foo of nlx.lib.bar" et al in general form, or
"function foo|bar|baz (...) {...}" where original is the first one "foo"
and the other synonyms all live in the same subdepot, and in particular the
others are "not" inner materials of "foo".
- Also add [Integer, Rational, Boolean], make [Int,Rat,Bool] into synonyms.
- Likewise (and necessarily), subdepots themselves can have synonyms.
- Also update tuple (and by extension) database types to add attribute
synonyms which semantically are lightweight virtual attribute maps that
simply make 2 attributes always-identical so only one ever needs storing
and no map function is required.  Not the same as material/sdp synonyms.
- Update system catalog and grammars to add support for routine parameter
aliases, built-in to the definitions of the routines; all names for a param
are defined in an array, that ordering being source-code-metadata, and the
first item in the list being the "primary" name.
Grammar can be "function foo (Int <-- topic|0 : Int, other|1 : Int) {...}".
This is not supported for param names in generic expr context except for
the shorthand "=>foo", so "=>1 is allowed".
- Change grammar so any number positionals supported for both s-d and u-d,
always map to "0","1".."N" and *not* "topic","other".
Also, any ".foo" now is short for "0.foo" rather than "topic.foo".
- Consider changing param names of special routines like value-filter etc,
or at least change any "topic" to "0" (other "1") so it works with ".foo".
- Update the documented signatures of all system-defined routines to use
the updated grammars reflecting the above additions.  Add param aliases of
"0" and "1" for every "topic" and "other" respectively, keeping said old
names too, and add other aliases as appropriate.  Add routine synonyms for
every distinct way of spelling a routine that rtn-invo-alt-syn provided, so
one can then always use that spelling in "foo(...)" plain-rtn-inv syntax;
update all routine docs so that the "also known as" comments no longer
mention any declared synonyms, no longer mention anything as "C<foo>" but
rather just anything as "I<foo>".
- Just stick to that, basically, leave anything else such as Unicode or
rtn-invo-alt-syn alone/not-removed for this release.

* Consider adding a midweight version of virtual-attribute-maps which is
like the fullweight version but that it expressly maps 1 attr to 1 attr; it
still uses a map function but that is no longer Tuple<--Tuple.

* Demote the "[array|set|etc]_of" types from a special concept knowable by
the backend (and explained in Basics.pod), where you can essentially use
some data types without them being declared as system catalog materials, so
that instead actual s-c materials *are* required; this syntax will remain
only as a dialect feature which is a shorthand for inline type definitions;
eg, these are now all equivalent:
    - param : set_of.Foo
    - param : relation-type { over tuple-type T { value : Foo } }
    - param : set-type over Foo
... or we might consider more material kinds specif to [set|array|etc]-type
so to help preserve the user's syntax and be more compact, maybe, but those
6 or so could probably be repr by single m-k which has an enum type attr.
Also thanks to the change about replacing N-adic with dyadic s-d routines,
and its precedent, there is less need for "foo_of" shorthands anyway.

* Externalize all the details of character string repertoires or encodings
from the Muldis D core, such that say all the details of Unicode become
part of a Muldis D extension instead, and maybe ASCII likewise.
More plans pending.

* Considering the following items where non-ASCII chars are much more
pervasive (though strictly optional), replace the "op_char_repertoire"
pragma with a pragma that affects all non-quoted code in general, including
all nonquoted (but not quoted) entity names.  The options would be, at
least, the 3: ASCII, Unicode_6.2.0_canon, Unicode_6.2.0_compat.  There
would separately be options for each kind of quoted character string:
quoted entity names, texts, comments; see later TODO item about this; as
per that, all of these could be part of a single pragma.  A simpler
implementation could support only ASCII across the board as literal
characters, while non-ASCII data could be supported as escape sequences.

* Look at this for a long list of Unicode gotchas / false assumptions, etc:
http://stackoverflow.com/questions/6162484/why-does-modern-perl-avoid-utf-8-by-default

* Enhance the cat-type/syntax for defining tuple types as attr lists (and
by extension, relations and scalar possreps) to let users provide an
optional hint for the order that tuple/sca-pr/etc attributes should be
consulted when doing an equality test between 2 tuples/etc so to direct the
DBMS to do the least expensive comparisons first, eg integer attributes,
prior to more expensive ones, eg blob attributes; since the test
short-circuits, and assuming the vast majority of compares would return
false, this should aid performance in a clean way without users resorting
to overload operators or something for performance reasons.  This is a
separate hint from that garnered by marking relation attrs as key attrs,
and could work within that eg to suggest order within multi-attr keys.
Other areas in the language could probably be assisted by hints also.

* Consider support for functions that aren't fully deterministic but for
which it is reaonable to cache their results.  For example, a function ...
THIS THOUGHT REMAINS UNFINISHED.

* Consider making the generic equality test operator virtual, so
user-defined scalar types can explicitly define it for themselves (or it is
generated if they don't explicitly say otherwise), but it is still
system-defined for tuples and relations.  The semantics of this operator
then are treated not so much as "is same" but rather as "is substitutable",
which is how the system would treat it.  This still needs a lot of thought.
To be more specific, "=(Int,Int)" and "=(Int,List)" are system-defined and
can't be overridden, but "=(List,List)" isn't at that level.
Perhaps the answer is for "=" to be defined at the lower type level only
and "=(Any,Any)" be undefined and a type-mismatch error as TTM suggests?
And rather certain other operators are more universal and over which "="
can be defined?  Maybe have multple =-like ops for different purposes.

* Consider adding native concepts of "value instance identifiers" (or
substitute "occurrence" or "sample" etc for "identifier") where these are
analogous to Perl "references", and provide system "functions" for
obtaining the VII of values, like Perl has "ref".  Note that given the same
instance X, multiple vii(X) must give the same result.
Especially in a system where the generic equality test operator is virtual,
VII can provide an implementation-influenced baseline something or other.
THIS THOUGHT REMAINS UNFINISHED.

* Taking further the idea of how various Text subtypes are defined, eg that
normalization is required, so that the Text generic equality operator would
"just work":  Consider defining that formally a Tuple or Relation etc value
has a canonical form in the low-level type system, meaning the member
attributes and tuples are always sorted in a specific system-defined way,
and so any operators that are sensitive to the low-level type system would
be fully deterministic, and the regular "=" operator would just work
without that having to be virtual or have special cases.  The only wrinkle
with that then relates to scalars and their multiple possreps.  Of course,
this is just the canon, but most normal operators wouldn't be sensitive to
this and so tuples/relations are still effectively unsorted, and more
important, an implementation doesn't actually have to store them sorted
normally, same as the "NFD" character strings don't actually have to be
stored that way by the system.  The canonical order is as follows: 1. Two
Int always sort as is normal for integers; 2. Two List always sort as is
normal for comparing strings, by comparing their elements pairwise; 3. An
Int always sorts before a List.  Further to this canonicalization, we no
longer offer multiple forms for Tuple and Relation; now, the payload of
each has 2 elements, heading and body, where the heading is sorted by
attribute name String and each tuple-body corresponds as usual, and a
relation body is a list of 1 element per tuple-body, and the elements of
that list are sorted by tuple-body List value.  So now the main thing to
figure out how to work this is in regard to multiple scalar possreps, so
that "=" has the correct semantics at the user level.  Ostensibly the
solution is quite simple, which is to mandate that a specific possrep is
the scalar type's canonical low-level form, and that moreover the name of
this possrep must be the empty string, meaning that if you actually want
that possrep to have a different name you must have a second possrep whose
form is identical to it.  A consequence of this is that, in the low-level
type system, every scalar type only has 1 possrep, and hence the possrep
name doesn't actually have to be stored, so then a scalar value then just
becomes a 2-element payload where the first is the scalar type name and the
second is a tuple value payload.  A benefit of all this canonicalization is
also that certain operations like a function to extract "a" tuple from a
relation can be fully deterministic, or similarly that we have a way of
finding a default "total sort order" for the entire type system that is
fully deterministic, even if not generally useful.  The function for this
low-level sorting, that works on Universal, is *disjoint* from the ordering
routines that users normally deal with, and is just intended for use in
canonicalization of List etc for "="; it is a plain real function like "="
and not a virtual like "<=>" etc.  Regardless, this whole paragraph
requires more thought.

* Update the PTMD_STD grammar to split up the "Name_payload" or its parts
further so that, rather than just the 2 "[|non]quoted_name_str", there is
at least the additional "nonquoted_rtn_invo_name_str" which is only allowed
to be used in a routine invocation context like <op><unspace>(...), with
trailing parenthesis, and not in a context lacking trailing parenthesis.  A
"nonquoted_rtn_invo_name_str" is a nonquoted string containing no
whitespace and, in addition to all the chars nonquoted_name_str allows,
also many other symbolic chars such that wouldn't confuse the parser, so
bracketing chars would likely be disallowed, at least as leading or
trailing characters in the string, and trailing colon could be disallowed,
and leading comma or leading => etc.  The idea here is that people can then
write "+(foo,bar)" for addition or "++(foo)" for increment, or "=(foo,bar)"
for comparison, "@(t)" or "%(r)", or ":=(target,value)" for assign.
In this case, if infix ops are allowed, they'd have to have mandatory
surrounding whitespace.
We also generally have to revisit Unicode for what is allowed in bareword
variable/etc names such as non-Latin or accented letters in general.  The
parser would have to use Unicode character classes in its definitions,
then.  Look at what Perl 6 does for some guidance.
As per another change, also assume that the idea of the internal catalog
no longer using Unicode for sys-def entity names is no longer true.
So Muldis D would then much more be Polish notation (with parens) by
default, and it should be much easier to just use the whole language that
way when it is more terse like this.  Supporting polish without parens
would be up to rtn-inv-alt-syn replacemnts while above is in plain-rtn-inv.
See also the 2nd(+?) next TODO item on splitting rtn-inv-alt-syn.
Also add yet another nonquoted...name_str that is just for use with
attribute/param/arg names and is only slightly less restrictive than the
old nonquoted_name_str in that it also allows strings of just or leading
digit chars; this is mainly so one can write positional params wo quotes.
Maybe just this last one can be added ASAP, and the other wait longer.

* Consider creating a branch of the Muldis D spec (and of the Muldis D
Manual) which retains all of the current spec features, and subsequently
strip out the whole rtn_inv_alt_syn catalog abstraction level in trunk so
that we can more radically evolve the language design at the more
fundamental level which plain_rtn_inv has access to, without worrying about
clashes or the complexity of a dozen-plus-precedence-level grammar.
Ideally the more fundamental level can evolve to the point that a
lot of what rtn_inv_alt_syn offers is no longer necessary in practice
with regards to making the code more terse.  The branch would merge in the
more fundamental changes with the old retained rtn_inv_alt_syn to see how
they might look together, or show how the new is absorbing the old; ideally
their differences would reduce over time without th branch losing features.
In the interest of marketing, the reduced trunk would retain all or much of
the example code using the then-removed features, as well as gain ones
using not yet specced features.  Each examples section would potentially be
split in 2, with the normal "Examples" just using the reduced spec features
and a new "Potential Future Examples" having anything not yet specced.
Also, the 3 Dialect files wouldn't actually lose the rtn_inv_alt_syn
precedence level but rather it would be made impotent as the grammar would
just define it as a non-proper superset of plain_rtn_inv for now; mainly
the change is that the 2 main pod sections "FUNCTION INVOCATION ALTERNATE
SYNTAX EXPRESSIONS" and "IMPERATIVE INVOCATION ALTERNATE SYNTAX STATEMENTS"
would be removed, or alternately stripped down to collection of "Potential
Future Examples" sections with a bit of commentary to explain if needed.

* The new version may be a lot easier to learn, considering that SQL + many
other C-like languages actually don't have too many non "f()" format ops.
Perhaps the main use of rtn_inv_alt_syn later is for people that want their
code to look like math/logic/etc exprs rather than named function calls.
IDEA:  Split rtn_inv_alt_syn into 2 abstraction levels where the lower one
has just 1-2 dozen or so plain prefix/infix ops such as
[:=, =,,!=, <,>,,<=,,>=,--,++, not,!,and,or,xor, +,-,|-|,*,/, ~, @,%,#]
and few are allowed having modifiers or that aren't in most languages.
Likely disallowed in lower level are [<=>,abs,div,mod,exp,^,**,log], the
other math ops, all other or Unicode variants of logic ops, all hyper-ops
including hypers of := or !, practically all relational/set/array/etc ops
including membership or sub/super tests.  As a middle-ground, for which we
could probably have a middle-third level from the split, are all the
postcircumfix ops that do restricted-to-constants shorthands of the likes
of array element access, projection, rename, un/group, un/wrap etc.
Things like the full set of infix logic ops are reserved for highest level,
and likewise for majority of Unicode ops and their ASCII-symbolic versions.
Now assuming we get generic <sym-op>(...) in plain-rtn-inv, and so
"+(foo,bar)" etc is an option, then we should reprioritize the above 3
post-split levels so that a level adding just postcircumfix syntax for
project/group/ary-acc/rename/etc should be the lowest additional level, so
one can be able to say "foo{...}" without also needing support for foo+bar.
Maybe call that new lowest "rtn_inv_pcfx_alt_syn".  Making postcircumfix
the lowest alt syn is also fitting because just it is like some of the
lower levels such as code-as-data where using some syntaxes make certain
inputs hard-coded, such as the attr names or interval-endpoint-flags,
versus those taking variables in the the more verbose generic syntaxes.
Presumably all levels higher than rtn_inv_pcfx_alt_syn are plain infix
or paren-less prefix with fully-variable arguments like generic functions.

* Consider making ASCII lookalikes for as many Unicode symbolic operators
as possible; for example:
 - join:  -> |X|
 - semijoin:  -> |X
 - semidiff:  -> |>
 - diff:  -> \
Well it's a thought anyway, though may not be worth the trouble in general.
If the backslash is allowed to be used this way, then we'll have to put
limits on its other uses for starting escape sequences, otherwise work out.

* Maybe this isn't feasible, but ...
Consider formally making every function map 1:1 from a tuple input to a
tuple output; it declares exactly 1 parameter that is a tuple type and its
result declared type is a tuple type.  Consider making every updater
formally do something analogous, such as having exactly 2 tuple-typed
parameters where only 1 is subject-to-update.  A recipe is like that but
has 4 tuple-typed parameters, 2 like updater and 2 global alias analogies.
A virtual attribute map kind of resembles this already.
Doing this would require making tuple attribute accessors special, their
own expression/etc node kind and not just a function ... though they kind
of are already as an alternative; also, variable assignment would have to
be a special node kind and not just an updater; in both cases, to save
their definitions from being mutually recursive.

Note that the first relational database system was the IBM IS/1, in 1970-2,
and the second one was IBM Peterlee Relational Test Vehicle (PRTV); the
latter's command language was Information Systems Base Language (ISBL).

----------

* In all 3 STD.pod, add code examples for each of these 4 material kinds:
scalar-type, domain-type, subset-type, mixin-type.

* In all 3 STD.pod, complete the description text, defining interpretation
in PTMD_STD and structure in the 2 Perl-STD, for each of these 7 material
kinds: scalar-type, tuple-type, relation-type, domain-type, subset-type,
mixin-type, subset-constraint.

* In all 3 STD.pod, populate the entire pod sub-section for each of these 2
material kinds, to provide concrete grammar, description text, and code
examples: distrib-key-constraint, distrib-subset-constraint.

----------

* Eliminate the simple monadic postfix special syntax category.  Convert ++
and -- into simple prefix ops, because an expression with
that in it is no longer end-weighted, and it would be less likely to
confuse people into thinking the op is variable increment rather than just
returning a result.  Removing the category also simplifies the parser as
there are no longer pre vs post precedence conflicts, and helps open the
door to the parser being more generic.  Simply eliminate postfix "!"
factorial or change it to prefix "fact".

* Update Basics.pod or other places to distinguish between the 2 main ways
that a type can be infinite, such as with "outwardly infinite" and
"inwardly infinite"; the later is when any 2 values have an infinite number
of others between them, so eg a time-of-day type could be infinite in the
inward sense but not in th outward sense; th result type of sin() likewise.
Also, the singleton types -Inf, Inf only refer to outwardly infinite types.

* Change the basic exception throwing mechanism from a function/procedure
to its own expression/statement node kind.  Call the new node kind "fail"
or "failure" or "throw" or "raise" something.  The "fail" node has a child
expression node or references a variable node which defines an Exception
value.  Simply evaluating a "fail" expression node will throw the exception
so a "fail" expr node is expected to only be the child of a short-circuit
expression like ??!!.
- Add a "fail" term, which throws a generic/default Exception value,
and/or a tight-binding "fail" prefix-keyword which takes an Exception arg;
that term/prefix is the concrete syntax for the new fail node.
- The "assertion" function can then go away; instead of writing
[$foo asserting $foo != 0], say [$foo = 0 ?? fail !! $foo].
- Add a few simple functions that each result in a kind of generic
Exception value.  At least have a niladic one for the most gen exception.
Then one could write [<cond-expr> ?? gen_exception() !! <expr-when-ok>].
- The treated() function then is just a wrapper over ??!! + isa.
- The fail() procedure will go away, replaced with a term/keyword also,
which maps to the "fail" statement node.
- Maybe use 'fail' for niladic term and 'raise' for prefix term?
- New keyword speelings:
    - failure
    - raised <expr>
    - fail
    - raise <var>
- Maybe alternatively, make an assertion into a lexical entity that is like
an expr node but doesn't have its own node name, and so is always used
either inline or offside, the main point being that users don't have to
come up with another node name when the node represents the same value as
another node and should naturally just have the same name.
Example:
    foo ::= ...
    asserts bar( foo )
    baz( foo )
... here, the assertion only happens when baz() is going to be evaluated;
the spelling is "asserts" since it should be an adjective.
- There also needs to be a version that can assert multiple exprs.
- Or actually, the ??!! version may still be better?
- Naming the "duplicate" isn't actually that hard; just use a leading
underscore, eg:
    _foo ::= foo asserting bar
    _foo ::= bar ?? foo !! failure
... so maybe that's best?
- A BIG THING TO CONSIDER HERE IS, HOW DO FUNCTIONAL LANGUAGES MAKE
ASSERTIONS ON COMBINATIONS OF ARGUMENTS ... OR IS THE ANSWER THAT ALL
FUNCTIONS HAVE EXACTLY ONE ARGUMENT?  SEE WHAT HASKELL/ETC DOES.

* Change generic assertion mechanism from a function/procedure to its own

* Add support for materials to have aliases.  But this kind of alias would
be simple, just an alternate unqualified name that exists in the same
namespace and is for the same material.  Aliases would be declared with an
"aliases" attribute, typed set-of-Name, held directly in the same catalog
types that have "name" attributes; for example, add it to the "FunctionSet"
type.  So, R.count becomes a simple alias for R.cardinality, and we can add
a whole bunch more aliases, so to make it friendlier for people who prefer
to call routines with foo(x,y) syntax rather than alternate symbols.  A
common use could be to provide both "prefix" and "infix" reading names,
such as both "product" and "multiply", and especially to give shorthands.
Example: "function product|multiply|mul (Int <-- x : Int, y : Int) {...}".
The first one in the list is the primary name, remainder are the aliases.
Or actually, it would probably be better for FunctionSet et al to *not*
internalize aliases, but rather have each alias exist as a separate
material which cites what it aliases.  And then that version could exist in
any public namespace (usually nlx), and not just the same subdepot as what
is being aliased.
The SYNONYM schema object of Oracle and other dbs corresponds to this, and
maybe "synonym" is what I should call mine too, being what the specific
material kind is called, leaving "alias" as a more generic term.
Even if we have separate synonym materials for routines/etc, one can still
declare them bundled into their originals like in the above foo|bar example
as that would just be a dialect shorthand but produce separate materials.
Also useful in support of users having their own home subdepots which have
aliases to the things they use, without them having to know where they are.
Add alias for every 'op' node 2nd element for a routine, meaning eg add
"+" and "" as aliases, and so then a Muldis D parser can then produce
calls to those, as if one said `"+"(4,5)` or `""(foo,bar)`, and so we can
better remember the individual syntactic choices that the users made.
But then, how do we deal with the idea of making logical-not into a meta-op
so that there is no actual is_not_same|"" function etc; how do we
preserve user's individual syntactic choices then?  So think about that.
While SQL synonyms can also be used for relvars, mine would probably only
be used for materials - types, routines, stim-resp-rules, themselves, etc;
perhaps leave relvar aliases to be handled by virtual attributes.

* With the improvements from having aliases or supporting "+"(x,y) etc, and
other language improvements, it becomes a lot more feasible for users to
settle for users to be satisfied with "plain_rtn_inv", that being
sufficiently terse, and so there is less need for "rtn_inv_alt_syn" to be
implemented or available.

* Maybe also treat material names like `function "infix<+>" (...) {...}` as
special such that if a parser encounters a random "foo + bar" then it would
parse it as if it were `"infix<+>"(foo,bar)` maybe I guess.  But if this is
going to work in a general sense, including for user-defined things, then
general format rules have to be set out for the parser so that if it sees
anything like X, without knowing what ops are declared, then it treats it
as an operator rather than some other construct.  On the other hand, we're
sure to run into trouble in trying to support non foo(x,y) syntax for
user-defined operators (besides those overloading system-defined virtuals),
and so better off just not doing this period; "infix<+>" is not special.

* Add support for routine parameters to have aliases, that is, for a named
parameter to be able to bind with a named argument where the argument may
have several possible names.  One use for this would be to support
parameters where it is desired to refer to them within their routine using
one name, but to use a different name in the argument, such as because the
latter is shorter or reads better (the Perl 6 spec should have some
examples of this).  Another use for this is to provide better support for
mixtures of arbitrary numbers each of positional and named routine
arguments; any parameters that would be reasonable to have a positional
argument would have 2 names, where one is an integer and one is text.  All
Muldis D grammars would be updated to no longer consider 'topic' and
'other' as special, which is a contrived notion, and instead consider
'0','1',... special.  And so, for all system-defined or user-defined
routines, any `op(foo,&bar,baz)` would be parsed into the same thing as
`op("0"=>foo,&"1"=>bar,"2"=>baz)`, and `.name` would be `"0".name`.  Now it
will so happen that "topic","other" will be commonly used in parameter
names, typically paired with "0","1" but we can now be a lot freer to name
parameters something more descriptive, such as "addends", and not
artificially make them topic/other simply so they support positional
syntax.  An idea for declaration syntax when aliases exist is to use the
"|" char; eg `function foo (Int <-- topic|"0" : Int, other|"1" : Int)`.
Of course, this complexity is only in param lists; arg lists are unchanged
and still are plain tuples with a single name per attr/arg.
For simplicity, a single param name will be more important than the others,
and only that would be its "expression node name" or "variable name" within
its routine, by which it must be referenced; therefore, the current
system catalog for declaring parameters can remain unchanged, and new
rtn-decl-type rtn-heading-attrs can be added to declare aliases.
Largely for flexibility, and correctness where they don't make sense,
parameters will never automatically have a number alias, but rather only
when the routine definer explicitly gives it one.
Of course, these aliases only apply to regular params, not global params.
One result of this change is that the Muldis D grammars will no longer
consider positional ro and rw args in separate spaces such that they can
appear in either order; now all positional args must be in the correct
mixed relative order, as there is only one "0", not one per ro and rw.

* Add special syntax for more ops:
    - ?#foo - "has 1+ elements" - is_not_empty(foo)
    - !#foo - "has zero elements" - is_empty(foo)
    - foo :=!# - assign_empty(&foo)
... and maybe rename underlying routines in the process.

* Update the mixins feature to add support for mixins that define
attributes that types can compose, whereby we support some approximation of
"specialization by extension" while still actually being just
"specialization by constraint".
Maybe also it could be said ...
A primary purpose of mixins is to help with managing software reuse, mainly
when multiple types have a number of attributes in common, a mixin can
define these and then the multiple types can compose that mixin.  A mixin
or type that composes a mixin can both add additional attributes of its own
to what the mixin defines, and the composer can add extra constraints over
the composed attributes like forcing a subtype.
Maybe also do ...
Support delegation / 'handles'; for example:
    - Name explic delegate to Text attr
    - maybe Blob, Text explic delegate to String attr
    - a ColoredCircle would delegate to both Color and Circle attrs?
This will all take some work to get right; not /all/ Rat/etc can be subst.
Probably *only* those operators that Rational/etc explicitly declares can
be delegated to Rat/etc by TAIInstant/etc.

* Replace many N-adic routines with dyadic ones, specifically
those whose definition is a repetition of a dyadic operation (so, 'sum' or
'join' etc yes but 'mean' no), which users then can invoke by way of a
reduction function if they want N-adic syntax.  Also let system catalog
store more information such as whether or not functions are commutative or
associative or idempotent or symmetric etc; likewise, the function def can
store what the operation's identity value is, if it has one, as meta-data,
useable when comm/assoc; the reduction func can read this using a
meta-programming function or something.  Reduction will fail if used on a
base func that doesn't define an identity if given an empty list.
The point of this change is to make the common dyadic case of N-adic
operators simpler, and also set a foundation for user-defined operators
that provide more information such that a compiler can be more effective
in optimizing them, or something.
The explicit/normal way, then, to indicate in code whether you want the
parser to produce a reduce op wrapper call rather than nested direct
invocations in the system catalog, is to just invoke the reduction
operator directly and explicitly pass an operand list; but the reduce op
would have special syntax, taking normal collection exprs, such as:
    [+] {5,23,5}
    [~] ['hello', 'world']
    [join] {order,inventory}
    [*] {1..5}
... or something.  Not using that would parse into nested dyadic calls
instead though the compiler can still rearrange.
Once we do that, its also simple to add hyper-operators, though arguably
these are redundant with 'map' or 'extension' etc.
Or this would be better for simplicity, given it won't be used as often,
and any dyadic infix function at all may be used, spelled the same way:
    reducing + {4,23,5}
    reducing ~ [...]
    reducing join {...}
    reducing * {...}
    reducing <nlx.lib.myfunc>(a=>3) {...}
... and so the regular operators can be parsed as usual.
Or maybe:
    reduced {4,23,5} using +
    reduced {...} using <nlx.lib.myfunc(a=>3)
... but that might have an end-weight problem?
Or, still go symbolic like the first one, but use prefix notation so that
it works well with both symbolic and wordy or inline-defined operators:
    []+ {5,23,5}
    []~ ['hello', 'world']
    []join {order,inventory}
    []* {1..5}
    []<nlx.lib.myfunc>(a=>3) {...}
Another consideration is that, when combined with routine synonyms that are
symbolic, the plain_rtn_inv alone would let you do this:
    reduce( <"+">(), {5,23,5} )
    reduce( <"~">(), ['hello', 'world'] )
    reduce( <join>(), {order,inventory} )
    reduce( <"*">(), {1..5} )
    reduce( <nlx.lib.myfunc>(a=>3), {...} )

* Furthering the above, add somewhat generalized support for what Perl 6
calls "meta" operators, at least in that we define and exploit several.
The general reducer above would be one of these.  Another is the negated
relational, whose syntax is putting ! or not- in front of any Bool-resu op.
Another is the assignment, putting := in front of any function.
For !, we can then eliminate all the "not" variants of any Bool-resulting
functions, so eg "x != y" parses into "not(is_same(x,y))", same as if
they had said "!(x = y)".  As for the old intended purpose of all the not-
variants, which is to preserve the user's intent of how code should look,
we could simply have an alias for the not() function which is what is
parsed into when != is used, and the old not() is just parsed into when the
separate prefix op is used.  On the other hand, while lots of not- variants
would go away, we'll keep the alias-but-param-order-reversed dualities such
as less-than/greater-than and sub-superset; unlike these, what we're
eliminating would not result in losing track of which args are lhs/rhs.
A related change is infix ops like  or  would parse into not(foo()) even
though they don't have the !; these would be aliases for the combos, same
as Perl 6 has != as an alias for !==.
For :=, we can eliminate all the updaters that are just shorthands for
doing an op and assigning the result to one of the args.  And so a
"foo :=union bar" would parse to "assign(&foo,union(foo,bar))".  Once
again, an alias for assign() can exist which such combos are parsed into,
where the regular assign() is used when users write "foo := foo union bar".
Of course, despite Muldis D requiring operator combos where singles used to
work, we assume that implementations will be smart enough to, say, use a
single "!=" or "insert into foo ..." etc when it sees the combination, so
there is no performance loss.
Probably, any meta'd operator would have the same precedence as the base
operator that it is modifying.
Adding the hyper-meta may not be useful since we already have map()/etc;
or alternately it might be useful in avoiding some uses of map or extend
or substitute etc where users are just adding/defining one attr.
Or maybe hyper-meta would only be useful with Set/Array/Bag because the
general map/extend/etc would require naming the attribute explicitly.
As for ASCII vs Unicode etc, that preference is never encoded in the system
catalog, so when code would be generated from the system catalog, it would
be up to the generator's configuration for which versions are used.

* Add hyper-meta in a more general fashion, as per the Ranked general type
of which Array is a more specific kind.  The hyper-meta is fundamentally
associated with the join operator, because it typically involves taking 2
relations, joining them on one set of same-named attrs (exactly 1 usually),
and then taking another set of *same-named* attrs and applying the hypered
op pairwise and deriving a single replacement set of those attrs with the
results.  The argument attrs would be renamed distinct first.  For example,
given 2 relations A{key,value,x} and B{key,value,y}, where we assume that
"key" is a unary key of each relation, the expression
"A >>+<< B" is roughly like this code:
  with (
    a ::= A{%others_a<-!key,value}{value_a<-value}
    b ::= B{%others_b<-!key,value}{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr{<-%others_a}{<-%others_b}
And the result is a relation with heading {key,value,x,y} but of course
with the more typical case the inputs and output are just {key,value}, in
which case that simplifies to:
  with (
    a ::= A{value_a<-value}
    b ::= B{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr
A variant taking a relation and a tuple would be like the >>+>> /etc form.
We might have variants for join vs union etc or generalize this further so
that bag/counted variants of relational ops can be defined using this
generalized hyper in combination with the regular relational ops, maybe.

* About extra metadata in the system catalog for functions/etc, see
http://www.postgresql.org/docs/9/static/extend.html for some ideas, such
as 35.13.x on Pg's use of COMMUTATOR and NEGATOR where function pairs
declare their complement operator.  The first pairs up "<" and ">" say (and
"+" pairs with itself) while the second pairs up "<", ">=" (dbl-chk that).

* Note that Pg exts are like Muldis D system modules in what they do, such
as that they add types and routines etc to the language.

* Change multi-update to be a sequence of statements rather than a set, and
explicitly allow the same target to be used more than once ... this could
be the case anyway thanks to virtual relvars etc.

* Move or adapt more Text functions into Stringy.
- Fundamentally all Stringy funcs work on Text in terms of the
"maximal_chars" possrep; this will just work correctly for when all
func args are of the same Text subtype, such as Canon etc.
- The Stringy/Text ops are analogous to Rational ops such that it is like
doing fraction math.  catenation() is like sum(), replication() is like
multiply, a substring test is related to difference/subtract (maybe "?~"
and "!~" might work as infix ops for something?).
- Move cat_with_sep to Stringy; semantics are clear cut and generalizable.
- has_substr ought to work with Stringy no problem from the Text and Array
perspectives, but Blob presents an issue purely concerning bit alignment,
such as whether we're searching on bits or on octet/etc alignments.

----------

* Update the virtual attributes maps so there is a way to manually specify
a reverse function, as meanwhile all the virtuals don't have to be either
read-only or updatable due to an automatically generated reverse function,
which might vary by implementation, which may be considered broken.  Note
that the reverse functions might have to be defined as per-tuple
operations, separately for insert/substitute/delete.

* Add new "material" kinds that define state constraints (address as simple
nlx.*.data.*), like type constraints but ref in reverse.

* Update the "material" kinds that def stimulus-response rules / triggered
routines so that they work for more kinds of stimuli, and maybe change the
keywords.  The material kind has 2 main attributes, where the "stimulus"
defines what to look out for and "response" defines what to do when the
former is sighted.  Some possible keywords for the first are "stimulus",
"cause", "when"; for the latter, "response", "effect", "invoke".

* Add new "material" kinds that define descriptions of resource locks that
one wants to get, starting with basic whole dbvar, relvar locks (address as
simple fed.data.foo.*, as well as simple relvar tuple locks (addr as prior
plus lists of values to match like with a semijoin); leave out generic
predicate locks at first but note they will be added later.
Update the system catalog concerning managing shared|exclusive locks or
looking for consistent reads between statements, etc.

* Large updates to docs concerning transactions and resource locking.
Note:  Supposedly PostgreSQL and MySQL use read-committed isolation by
default while SQLite provides serializable.

* Rewrite the "Exception" catalog type so it can carry metadata on what
kind of exception occurred, not just that an exception occurred.

* Also study SQL concept of conditions and handlers, looks sort of like
something between exception handling, signals; or it is their exceptions.

* Also adapt something like Postgres' LISTEN/NOTIFY/UNLISTEN feature, which
is an effective way for DB clients to be sent signals, such as when a
database relvar has changed.

* Use a conceptual framework for database transactions that is strongly
inspired by how distributed source-code version control systems (VCSs)
work, in particular drawing on GIT specifically.  The fundamental feature
of the framework is that the DBMS is managing a depot consisting of 1..N
versions of the same database, where every one of these versions is both
consistent and durable.  Each version is completely defined in isolation,
conceptually, and so any versions in a depot may be deleted without
compromising each of the other versions' ability to define a version of the
entire database.  It is implementation-dependent as to how the versions are
actually stored, such as each having all of the data versus most of them
just having deltas from some other version; what matters is that each
version *appears* to be self-contained.  Every version is created as a
single atomic action, and it is never modified afterwards, though it may be
later deleted (also an atomic action).  Every in-DBMS user process,
henceforth called "user", has its own concept of the current state of the
database, which is one of the depot's versions that is designated a "head".
A user's current head is never replaced during the course of the in-DBMS
process unless the user explicitly replaces it, such as by either
performing an update or requesting to see the latest version (the latter
done such as with an explicit "synchronize" control statement).  Therefore,
each user is highly isolated from all the others, and is guaranteed
consistent repeatable reads and no phantoms; they will get repeatable reads
until they request otherwise.  The framework has no native concept of
"nesting transactions" or "savepoints" or explicit "commit" or "rollback"
commands.  Rather, every single DBMS-performed parent-most multi-update
statement (which is the smallest scope where TTM requires the database to
be consistent both immediately before and immediately after its execution),
is a durable atomic transaction all by itself.  The effect of a successful
multi-update statement is to both produce a new (durable) version in the
depot and to update the executing user's "head" to be that new version (the
prior version may then be deleted automatically depending on
circumstances); a failed multi-update statement is a no-op for the depot,
and the user gets a thrown exception.  A depot's versions are arranged in a
directed acyclic graph where each version save the oldest cites 1..N other
versions as its parents, and conversely each version may have 0..N
children.  A child version has exactly 1 parent when it was created as the
result of executing a multi-update statement in the context of the parent
version; the parent version is the pre-update state of the database and the
child is the post-update state of the database.  A child version has
multiple parents when it is the result of merging or serializing the
changes of multiple users' statements that ran in parallel.  One main
purpose of tracking parents like this is for reliable merging of parallel
changes, so that the intended semantics of each change can be interpreted
correctly, and potential conflicts can be easily detected, and effectively
resolved.  More on how this works follows below.  Note that versions simply
have unique identifiers to be referenced with and there is no implied
ordering between them if they are generated as serial numbers or using date
stamps, though versions with earlier date stamps are given priority in the
case of a merge conflict.  So a multi-update statement is the only native
"transaction" concept, and it is ACID all by itself.  Now, the
multi-statement "transactions" or concepts of nested transactions or
savepoints would all be syntactic sugar over the native concept, and
basically involve keeping track of versions prior to the head and
optionally making an older one the head.  This framework uses the VCS
concept of "branching" (which is something that GIT strongly encourages the
use of, as GIT makes later "merging" relatively painless) as the native way
to manage concurrent autonomous database updates by multiple users.  By
default, when no users have made any changes to the database, a depot just
has a "trunk", and its childmost or only version is called "master"; every
database user process' "head" starts off as the "master" version when that
process starts.  Each (autonomous) user process that wants to update the
database will start by creating a new branch off of the trunk, and
subsequent versions of theirs will go into that, rather than into the trunk
or some other branch.  The trunk is shared by all users while each user's
branch is just for that user, as their private working space.  Note that,
unlike a VCS in general where branches can become long-lived and interact
with each other independently of the trunk, the framework instead follows
the typical needs of an RDBMS, which espouses a single world view as being
dominant over any others, and expects that any branches will be very
short-lived, not existing for longer than a conceptual "database
transaction" would; only the trunk is expected to be long-lived.  (This
isn't to say that a DBMS can't maintain them long term, but one that acts
like a typical RDBMS of today wouldn't.)  Note that the final action on a
branch that involves merging into the trunk, this would be perceived by all
other DBMS users as all of the changes wrought by the branch being a single
atomic update, though the user performing it may see several steps.

* Flesh out matters related to starting or communicating between multiple
autonomous in-DBMS processes, in general, besides the special case about
sequence generators.

----------

* Add to Routines_Catalog.pod and other files
definitions of any remaining routines, eg String routines, that would be
needed so that for all system-defined types all the necessary
system-defined routines would exist that are necessary for defining said
types, especially their constraint or mapping etc definitions.  So in
String.pod we need [catenation, repeat, length, has_substr] etc.
Also add "is_coprime" or GCD or LCM or etc which are used either in the
constraint definition of Rat or in a normalization function for Rat; see
also "the Euclidean algorithm" as an efficient way to do the calculations.

* Consider adding type introspection routines like: is_incomplete() or
is_dh() or is_primitive|structure|reference|enumerated etc.  Or don't
since one could look that up in the system catalog.  But more tests on
individual values might be useful, or maybe we have enough already.

* Add ext/TAP.pod, which is a partial port of Perl 5's Test::More / Perl
6's Test.pm / David Wheeler's pgTAP to Muldis D; assist users in testing
their Muldis D code using TAP protocol.  The TAP messages have type Text.

----------

* Add concept of shallowly homogeneous / sh- relation types to complement
the deeply version, and named maximal types like SHRelation, SHSet,
SHArray, etc to complement the DH/etc, and sh_set_of/etc to complement
dh_set_of/etc; but not sh-scalar or sh-tuple as the concept doesn't make
sense there.  Then update functions like Relation.union/etc to take
sh_set_of.Relation rather than set_of.Relation, which more formally defines
some of their input constraints.

* Consider adding an imperative for-each looping statement; the main
question here is whether it should work on any (unordered) relation or just
on an Array (in which case it iterates through the tuples in sequence by
index); the question is what tasks the for-each would be used for; perhaps
both versions are useful; presumably the main reason to have for-each at
all is when I/O is involved and some derivative needs to be output either
where order matters or where order does not matter; but perhaps only a
routine is needed here such as a catenate function plus normal I/O output.
The question also is what tasks would an imperative for-each be needed for
that functional constructs like the list-processing relational functions
can't better be used for those tasks instead.

----------

* Add a round-rule param to rat division, I suppose, since in general we'll
need it if we want to maintain a rational radix through every op (+,-,*
will already do so when all their args are in the desired radix).

* Add explicit support for +/- underflow, +/- overflow, NaNs, etc.
I'm inclined to think +/- zero is unnecessary when we have underflow and
can be confusing anyway (just a single normal number zero is better).
I'm not sure if +/- overflows are useful or if infinities cover them for
our purposes.  How this would work is that we define a set
of scalar singleton types, one for each of the special values.  Then we
define extended versions of the Int, Rat, etc types where the extended
types are defined in terms of being union types that union the regular
numeric types with the special singleton type values.  This approach also
means just one each of +Underflow, -Overflow, etc is needed and is a member
of extended Int or Rat etc.  Consider using the existing names "Int"/"Rat"
with the versions that include these special values, and make new names for
the current simpler versions that don't, such as "IntNS" (int no specials),
"RatNS", etc.  Either way, it is useful to support the full range of values
that a Perl 6 numeric can support, or that an IEEE float can support,
without users necessarily having to define it themselves.
IDEA:  Maybe make all normal math/etc ops work with the extended versions
(those with NaNs, infinities, etc) and in situations where users don't want
those special values they just use a declared type excluding them, and then
the normal type constraints will take care of throwing exceptions when one
divides by zero for example.

* Flesh out Interval.pod to add a complement of functions for comparing
multiple intervals in different ways, such as is-subset, is-overlap,
is-consecutive, etc, as well as for deriving intervals from a
union/intersect/etc of others, as well as for treating intervals as normal
relations in some contexts, such as for joining or filtering etc, as well
as a function or 3 to do normalization of Interval values.
Maybe the type name 'Range' can be used for something.
Maybe the type name 'Span' or 'SpanSet' can be used for something;
there are Perl modules with those names concerning date ranges.
Input is welcome as to what interval-savvy functions Muldis D should have.

* Consider renaming Interval to Range, even if that is less specific,
for brevity, and also so we have a word that looks less like Integer
or that an Int abbreviation would be less ambiguous.
- Update, upon discussion, I decided to stay with "interval", which both
Hugh, Philip, Derek agree is the best term, with none arguing differently.

* Flesh out some window/partition funcs, which are kind of like a
generalization of aggregation/reduction functions.  A window()/partition()
wrapper func is like the summary() wrapper func but it has the same number
of output tuples as input ones; when wrapping an agg/reduc func, all output
tuples have the same value per tuple in the same group; when wrapping a
window/partition-oriented func, such as rank(), each tuple in the group
gets or can get a different value.
See these:
- http://www.postgresql.org/docs/9.0/interactive/tutorial-window.html
- http://www.postgresql.org/docs/9.0/interactive/functions-window.html
- http://www.postgresql.org/docs/9.0/interactive/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS

* IN PROGRESS ...
Add Bool-resulting relational operators EXISTS and FORALL, that provide
"existential quantification" and "universal quantification" respectively,
these being useful in constraint definitions.  See TTM book p168, pp394-5
for some info on those.  Also add analogies to Perl 5's List::MoreUtils
operators any(), all(), notall(), none(), true(), false(); some of those
may be the same as EXISTS/FORALL.  Also add an EXACTLY operator like the
Tutorial D language has, and a one() op that is between any() and none().
Maybe some pure boolean ops can be added analogous to the above also; eg
any() an alias for or() and all() an alias for and().
is_(any|all|one|none|notall|etc)_of_(restr|semijoin|semidiff|etc)
source is any|etc matching|where|etc filter|etc
ADD RELATIONAL OPERATORS THAT COMBINE BOOL OPS ADDED IN 0.80.0 WITH
RELATIONAL MAP/RESTRICTION/ETC AND ... The new functions are modelled after
some in Perl 5's List::MoreUtils module.
That is, add prefix ops exactly|all|any|one|none|etc
which take a relation and result in True or False depending on what that
relation's cardinality is.  In some cases, an extra arg is needed:
    - exactly((st),n) = (#(st) = n)
    - none((st)) = exactly((st),0) = !#(st)
    - any((st)) = !exactly((st),0) = ?#(st)
    - all((st),#s) = exactly((st),#s) = (#(st) = #s)
    - notall((st),#s) = !exactly((st),#s) = (#(st) != #s)
    - one((st)) = exactly((st),1) = (#(st) = 1)
OR MAYBE THESE AREN'T ANY MORE USEFUL THAN THEIR EQUIVALENT EXPRS.

* Consider adding sequence generator updaters|procedures in Integer.pod.

* Consider adding random value generators for data types other than integer
and rational numerics, such as for character strings or binary strings.

* Consider analogy to SQL's "[UNION|EXCEPT|INTERSECT] CORRESPONDING BY
(attr1,attr2,...)", which is a shorthand for combining projection and
union, that takes a list of attributes and unions the projections of those
attributes from every input relation; so this means, as with join(), that
the input relations don't need to have the same headings.

----------

* In PTMD_STD, consider further changes to how character escape sequences
in strings/etc are done.  For example, whether the simple escape sequence
for each string delimiter char may be used in all kinds of strings (as they
are now) or just in strings having the same delim char as is being escaped.

* IN PROGRESS ...
Update the STD dialects to support inline definition of basic
routines (and types?) right in the expressions/etc where they are used,
such as filter functions in restriction() invocations, so many common cases
look much more like their SQL or Perl counterparts, or for that matter, a
functional language's anonymous higher order functions.  This syntax would
be sugar over an explicit material definition plus a FooRef val selection,
which means the inner def effectively is an expression node, and users can
choose to name or not name the FooRef selecting node as normal with value
expressions.  It is expected that the materials could be decl anonymously
and names for them (the inn.foo, not the FooRef's lex.foo) would be
generated as per inline expression nodes etc.

* Further to the previous item, add some special syntax, similar to how one
references a parameter to get its argument's value, which can see into the
caller's lexical scope.  This would be sugar over declaring parameters with
the same name and having the caller explicitly pass arguments to it,
without having to explicitly write that.  Generally this syntax would only
be used with inline-declared routines.  But similarly, add some special
syntax allowing one to essentially just write the body of a routine without
having to explicitly write its heading / parameter list, which is useful
for routines invoked directly from a host language, where said parameters
are attached to host bind variables.  Now one still has to say what the
expected data type is for these bind variables, but then the explicit
syntax for such Muldis D routines is more like that of a SQL statement you
plug into DBI or whatever, without the explicit framing.  May not work
anywhere, but should help where it does.  Maybe use $$foo rather than $foo
to indicate that the 'foo' wasn't explicitly declared in the current
lexical scope and we are referring to the caller or a bind variable.  Or
rather than $$foo, have something like "(param foo : Bar)" for an
expression-inline parameter definition and use, where the part after the
"param" has all the same syntax as an actual param list; this is the one
for host language bind parameters.  Actually that might be useful by
itself.  Similarly "(caller foo)" would be the look to parent Muldis D
lexical scope, or $$foo would just do that maybe, unless this should have
an explicit type declaration still.  Note, if same inline-declared host
param used more than once, you just need "(param foo : Bar)" form once and
other uses can just say foo as per usual; in fact, it must be this way.

* Consider in all STD adding a new pragma that concerns whether data in
delimited character string literals is ASCII or Unicode etc.
Example PTMD_STD grammar additions:
            <ws>? ',' <ws>? str_char_repertoire <ws>? '=>' <ws>? <str_cr>
    <str_cr> ::=
        '{' <ws>?
            [<str_cr_describes> <ws>? '=>' <ws>? <str_char_reper>]
                ** [<ws>? ',' <ws>?]
        <ws>? '}'
    <str_cr_describes> ::=
        all | text | name | cmnt
    <str_char_reper> ::=
          ASCII
        | Unicode_6.2.0_canon
        | Unicode_6.2.0_compat
Example PTMD_STD code additions:
    str_char_repertoire => { text => Unicode_6.2.0_canon,
        name => Unicode_6.2.0_compat, cmnt => Unicode_6.2.0_compat },
    str_char_repertoire => { all => ASCII },
Of particular interest is the Unicode canonical vs compatibility, that is
NFC|D vs NFKC|D; it is generally recommended such as by the Unicode
consortium to use canonical for general data but to use compatibility for
things like identifiers or to avoid some kinds of security problems; see
http://www.unicode.org/faq/normalization.html.  Note that compatibility is
a smaller repertoire than canonical, so converting from the latter to the
former will lose information.  The text|name affect how delimited char
strs that are Text|Name are interpreted, and the effects are
orthogonal to whether characters are specified literally or in escaped
(eg "\c<...>" form); canonical will preserve exactly what is stated (but
for normalization to NFD) and compatibility will take what is stated and
fold it so semantically same characters become the same codepoints (like as
normalizing to NFKD).  The suggested usage is compatibility for Name to
help avoid security or other problems, and canonical for Text; as for
comments, I currently don't know which is better.  If ASCII is chosen, the
semantics are different; with both Unicode any input is accepted but folded
if needed; for ASCII, it is more likely an exception would be raised if
there are any codepoints outside the 0..127 range in character strings.
The 'all' is a shorthand for giving the same value to all 3 text|name|cmnt
and is more likely to occur with ASCII but it might happen otherwise.
An additional reason to raise this feature is to setup support for other
char sets in future, such as Mojikyo, TRON, GB18030, etc which go beyond
Unicode eg no Han-unification (see http://www.jbrowse.com/text/unij.html +
http://www.ruby-forum.com/topic/165927) but type system also needs update.

* Update HDMD_Perl6_STD.pod considering that a 2010.03.03 P6Syn update
eliminated the special 1/2 literal syntax for rats and so now one writes
<1/2> instead (no whitespace allowed by the '/'); now 1/2 could still work
but now it does so using regular constant folding and so having a higher
precedence op nearby affects its interpretation.

* Update HDMD_Perl6_STD.pod considering names of Perl collection types,
such that "Enum" is the immutable "Pair" and "EnumMap" was renamed from
"Mapping", and "FatRat" is now the "Rat" of unlimited size, etc.

* Consider using postcircumfix syntax for extracting single relation
attrs into Set or Bag etc, meaning wrap_attr; eg "r.@S{a}", "r.@B{a}".
Now that might not work for Array extraction, unless done like
"(r.@A{a} ordered ...)" or some such, which isn't pure postcircumfix,
but that may be for the best anyway.

* Consider adding concrete syntax that is shorthand for multiple
single-attribute extractions where each goes to a separate named expression
node (or variable) but the source is a single collection-typed expr/var.
Or the source could be a multiplicity as well, or mix and match.
The idea here is to replicate some common idioms in Perl such as
"(x, y) = @xy[0,1]" or "(x, y) = %xy{'x','y'}", this being more useful
when the source is an anonymous arbitrary expression.
Proposed syntax is that, on each side of the "::=" or ":=", the source and
target lists are bounded in square brackets, indicating named items assign
in order, and syntax for collections supplying/taking multiple items are
ident to single-attr accessors (having a ".") but that a list is in the
braces/brackets; for example: "[x, y] ::= [3, 4]",
"[a, b] ::= t.{a,b}", "[c, d] ::= ary.[3,5]".  This syntax would
resolve into multiple single-attr accessors when app in system catalog.
The assignment variants of the above would naturally fall out the ability
to have arbitrary expressions on both sides of the ":=", so what you do is
have an array-valued expression on both sides, eg "[x,y] := [y,x]" works
because "[...]" is an array literal now.
We can overload ".[]" for tuples in general so they extract like projection
but return an array rather than a tuple, so we can then say
"[a,b] ::= t.[a,b]" or even "t1.[x,y] := t2.[a,b]" to multi-substitute,
that being a shorthand for "t1.x := t2.a, t1.y := t2.b".  We can't do that
for general relations though since the array subtype of rel is using it.
This mechanism also provides a general way for a function to have multiple
ord retv; eg, "[x,y,z] := foo(...)"; like Perl's "($x,$y,$z) = foo(...)".
A variable (or subject-to-update parameter), "bar", may be aliased using
"foo ::= bar" such that "foo" is an expr node, but like all named exprs in
procedures, "foo" is conceptually reevaluated per mu-statement.
Ordered tuples can be used instead of arrays, and in fact might be a better
solution for multiple reasons.  To do this, just say "%:{x,y,z}" rather
than "[x,y,z]"; the former is shorthand for '%:{"0"=>x,"1"=>y,"2"=>z}'.

* In PTMD_STD, consider loosening the grammar regarding some of the normal
prefix or postfix or infix operators so that rather than mandating
whitespace be present between the operators and their arguments, the
whitespace is optional where it wouldn't cause a problem.

----------

* Restore the concept of public-vs-private entities directly in sub|depots.

* Restore the concept of "topic namespaces" (analogous to SQL DBMS concept
of "current database|schema" etc) in some form if not redundant.

* Update the system catalog to deal with database users and privileges etc.

----------

* IN PROGRESS ...
A Muldis D host|peer language can probably hold lexical-to-them variables
whose Muldis D value object is External typed, and so they could
effectively pass around an anonymous closure of
their own language.  Such a value object would be a black box to the host
and can't be dumped to Muldis D source code.

* IN PROGRESS ...
Fully support direct interaction with other languages, mainly either peer
Parrot-hosted languages or each host language of the Muldis D
implementation.  Expand the definition of the "reference" main type
category (or if we need to, create a 5th similarly themed main category) so
that it is home to all foreign-managed values, which to Muldis D are simply
black boxes that Muldis D can pass around routines, store in transient
variables, and use as attributes of tuples or relations.  These
of course can not be stored in a Muldis D depot/database, but they can be
kept in transiant values of Muldis D collection types which are held in
lexical variables by the peer or host language; that language is then
really just using Muldis D as a library of relational functions to organize
and transform its own data.  We also need to add a top level namespace by
which we can reference or invoke the opaque-to-us data types and routines
of the peer or host language.  This can not go under sys.imp or
sys.anything because these are supposed to represent user-defined types and
routines, which in a dynamic peer language can appear or disappear or
change at a moment's notice, same as in Muldis D; on the other hand, types
or routines built-in to the peer/host language that we can assume are as
static as sys.std, could go under sys.imp or something.  This also doesn't
go under fed etc since fed is reserved for data under Muldis D control and
only ever contains pure s/t/r types.  Presumably this namespace will be
subdivided by language analogously to sys.imp or whatever syntax Perl 6
provides for calling out into foreign languages co-hosted on Parrot.  Since
all foreign values are treated as black boxes by Muldis D, it is assumed
that the Muldis D implementation's bindings to the peer/host language will
be providing something akin to a simple pointer value, and that it would
provide the means to know what foreign values are mutually distinct or
implement is_same for them.  One thing for certain is that every
foreign value is disjoint from every Muldis D value, and by default every
foreign value is mutually distinct from every other foreign too, unless
identity is overloaded by the foreign, like how Perl 6's .WHICH works.
The foreign-access namespace may have a simple catalog variable
representing what types and routines it is exposing, but to Muldis D this
would be we_may_update=false.

* IN PROGRESS ...
About External type ... update Perl5_STD and
Perl6_STD to add a new selector node kind 'External' which takes any Perl
value or object as its payload; this is treated completely as a black box
in general within the Muldis D implementation.  For matters of identity
within the Muldis D envirnment, it works as follows:  Under Perl 6, the
Perl value's .WHICH result determines its identity.  Under Perl 5, if the
value is a Perl ref ('ref obj' returns true) then its memory address is
used, and this applies to all objects also (since all refs are mutable,
this seems to be the safest bet); otherwise ('ref obj' is false) then the
value's result in a string context, "obj", is used as the identity; the
mem addr and stringification would both be prefixed with some constant to
distinguish the 2 that might stringify the same.  By default, an
External supports no operators but is/not_same.

----------

* Add new "FTS" or "FullTextSearch" extension which provides weighted
indexed searching of large Text values in terms of their component tokens,
such as what would be considered "words" in human terms.  This is what
would map to the full text search capabilities that underlying SQL DBMSs
may provide, if they are sufficiently similar to each other, or there might
be distinct FTS extensions for significantly different ones?

* Add new "Perl5Regex" extension which provides full use of the Perl 5
regular expression engine for pattern matching and transliteration of Text
values.  Maybe the PCRE library can implement this on other foundations
than Perl 5 itself if they are sufficiently alike; otherwise we can also
have a separate "PCRE" extension.  Or the same extension can provide both?

* Add new "Perl6Rules" extension which provides full use of the Perl 6
rules engine for pattern matching and transliteration of Text values.

* Add new "PGE" or "ParrotGrammarEngine" extension, or whatever an
appropriate replacement is, for pattern matching and transliteration of
Text values.  This and "Perl6Rules" may or may not be sufficiently similar
to combine into one extension.

* Add functions for splitting strings on separators or catenating them with
such to above extensions or to Text.pod as appropriate.  Text has one now.

* Update or supplement the order-determination function for Text so that it
compares whole graphemes (1 grapheme = sequence starting with a base
codepoint plus N combining codepoints, or something) as single string
elements, rather than say comparing a base char against a combining char.

* Add new "Complex" extension which provides the numeric "complex" data
types (each expressed as a pair of real numbers with at least 2 possreps
like cartesian vs polar) and operators.  Note that the SQL standard does
not have such data types but both many general languages as well some
hardware CPUs natively support them.  Probably make "Complex" a mixin type
and have the likes of "RatComplex" and "IntComplex" composing it.  Note
that a complex number over just integers is also called a Gaussian integer.
A question to ask is whether a distinct "imaginary" type is useful; some
may say it is and Digital Mars' "D" has it, but I don't know if others do.
In any event, complex numerics should most likely not be part of the core,
even though their candidacy could be considered borderline; for one thing,
I would expect that most actual uses of them would work with inexact math.

* Add other mathematical extensions, such as ones that add trigonometric
functions et al, or ones that deal with hyperreal/hypercomplex/etc types,
or ones with variants of the core numeric types that propagate NaNs etc.

* Consider adding a sleep() system-service routine, if it would be useful.

* Add multiplication and division operators to the Duration types; these
would both be dyadic ops where the second op is a Numeric.

* Consider adding a Temporal.pod type specific to representing a period in
time, maybe simply as an alias for 'interval_of.*Instant' or some such.
See also the PGTemporal project and its 'Period' type.

* Flesh out "Spatial" extension; provide operators for the spatial data
types, maybe overhaul the types.

* Consider another dialect that is JSON ... like HDMD in form, but stringy.

* Fundamentally, a Muldis D DBMS API or client-server protocol has a
command pattern where the request is Muldis D code and so is the response.
This is like the relationship between Javascript and JSON, where the data
is expressed in the same syntax as code.  PTMD_STD by default.
Actually, this brings up an interesting thought in that a DBMS shell could
be analagous to writing HTTP requests manually like telnet to port 80.
But a different Muldis D dialect would be optimized for machine-to-machine
like client-server stuff.
If we ignore parameter binding, a programmer API could also be a function
or procedure call that takes Muldis D code as a single argument and has
Muldis D code as the result; the argument is code defining a tuple value.

* Mention in the DBMS or learn from "Cego" (http://www.lemke-it.com/).

* Apparently Postgres has no built in scheduler feature, and
PgAgent is designed to cover that need.

----------

* Add one or more files to the distro that are straight PTMD_STD code like
for defining a whole depot (as per the above) but instead these files
define all the system entities.  Or more specifically they define just the
interfaces/heads of all the system-defined routines, and they have the
complete definitions of all system-defined types, and they declare all the
system catalog dbvars/dbcons.  In other words these files contain
everything that is in the sys.cat dbcon; anything that users can introspect
from sys.cat can also be read from these files in the form of PTMD_STD
code, more or less.  The function of these files is analogous to the Perl 6
Setting files described in the Perl 6 Synopsis 32, except that the Muldis D
analogy explicitly does not define the bodies of any built-in routines.  An
idea is that Muldis D implementations could take these files as is and
parse them to populate their sys.cat that users see; of course, the
implementations can actually implement the routines/types as they want.
Note that although this Muldis D code would be bundled with the spec, it is
most likely that the PTMD_STD-written standard impl test suite will not.
Note that these files will not go in lib/ but in some other new dir.  Note
that it is likely any implementation will bundle a clone of these files
(suitably integrated as desired) rather than having an actual external
dependency on the Muldis::D distro.  Note that some explicit comment might
be necessary to say there are no licensing restrictions on copying this
builtins-interfaces-defining code into Muldis D implementations, or maybe
no comment is necessary.  Probably a good precedent is to look at what
legalities concern existing tutorial/etc books that have sample code.

* Create another distribution, maybe called Muldis::D::Validator, which
consists essentially of just a t/ directory holding a large number of files
that are straight PTMD_STD code, and that emit the TAP protocol when
executed.  The structure and purpose of this collection is essentially
identical to the official Perl 6 test suite.  A valid Muldis D
implementation could conceivably be defined as any interpreter which runs
this test suite correctly.  This new distro would be a "testing requires"
external dependency of both Muldis::Rosetta and any Parrot-hosted language
or other implementation, though conceivably either could bundle a clone of
Muldis::D::Validator rather than having an actual external dependency.
This test suite would be LGPL licensed.  This new distribution would have a
version number that is of X.Y.Z format like Muldis::D itself, where the X.Y
part always matches that of the Muldis D spec that it is testing compliance
with, while the .Z always starts at zero and increments independently of
the Muldis D spec, as often there may be multiple updates to ::Validator
for awhile between releases of the language spec, and also since .Z updates
in the language spec only indicate bug fixes and shouldn't constitute a
change to the spec from the point of view of ::Validator.
